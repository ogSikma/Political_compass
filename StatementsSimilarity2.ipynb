{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102b5be7-21d7-44f6-9c58-613984460e7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in i:\\anaconda\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in i:\\anaconda\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in i:\\anaconda\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in i:\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in i:\\anaconda\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in i:\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in i:\\anaconda\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in i:\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in i:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: sentence-transformers in i:\\anaconda\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: pocketsphinx in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (5.0.3)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pocketsphinx) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in i:\\anaconda\\lib\\site-packages (from sounddevice->pocketsphinx) (1.17.1)\n",
      "Requirement already satisfied: pycparser in i:\\anaconda\\lib\\site-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.21)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (3.11.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in i:\\anaconda\\lib\\site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in i:\\anaconda\\lib\\site-packages (from SpeechRecognition) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.12.14)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (0.2.14)\n",
      "Requirement already satisfied: tf-keras in i:\\anaconda\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in i:\\anaconda\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in i:\\anaconda\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in i:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in i:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in i:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in i:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in i:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: pyttsx3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (2.98)\n",
      "Requirement already satisfied: comtypes in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (1.4.8)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in i:\\anaconda\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Requirement already satisfied: sentence-transformers in i:\\anaconda\\lib\\site-packages (3.4.1)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.4.1\n",
      "    Uninstalling sentence-transformers-3.4.1:\n",
      "      Successfully uninstalled sentence-transformers-3.4.1\n",
      "Successfully installed sentence-transformers-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn\n",
    "!pip install sentence-transformers\n",
    "!pip install pocketsphinx\n",
    "!pip install SpeechRecognition\n",
    "!pip install pyaudio\n",
    "!pip install tf-keras\n",
    "!pip install pyttsx3\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4910d030-2992-4c12-ac41-e5e474bba127",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Słucham...\n",
      "halo halo ha\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3 as tts\n",
    "\n",
    "r = sr.Recognizer()\n",
    "engine = tts.init()\n",
    "engine.setProperty('rate',125)\n",
    "\n",
    "def talking(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def getText():\n",
    "    with sr.Microphone() as source:\n",
    "        try:\n",
    "            print(\"Słucham...\")\n",
    "            audio = r.listen(source)\n",
    "            #text = r.recognize_sphinx(audio, language='pl')\n",
    "            text = r.recognize_google(audio, language='pl-PL')\n",
    "            if text != \"\":\n",
    "                return text\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "while True:\n",
    "    txt = getText()\n",
    "    if not txt == 0:\n",
    "        print(txt)\n",
    "        talking(txt)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Nic nie udało się rozpoznać...\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bff25ff-9197-429c-8ac7-6bea1cedecd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proszę mówić...\n",
      "Błąd podczas korzystania z pocketsphinx: missing PocketSphinx language data directory: \"C:\\Users\\Sikma\\AppData\\Roaming\\Python\\Python312\\site-packages\\speech_recognition\\pocketsphinx-data\\pl\"\n"
     ]
    }
   ],
   "source": [
    "# Funkcja do rozpoznawania mowy z mikrofonu\n",
    "def speech_to_text_offline():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Proszę mówić...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "        \n",
    "        try:\n",
    "            # Użycie pocketsphinx do rozpoznawania mowy offline\n",
    "            text = recognizer.recognize_sphinx(audio, language='pl')\n",
    "            print(\"Rozpoznany tekst:\", text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Nie rozpoznano mowy\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Błąd podczas korzystania z pocketsphinx: {e}\")\n",
    "\n",
    "# Wywołanie funkcji\n",
    "speech_to_text_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8f10b4-d33b-4dcc-9035-b8d5478fc984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo halo ha\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3c4f05-b9bc-494b-b607-99c067b26ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From I:\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from transformers import AutoModel, AutoTokenizer\n",
    "import json\n",
    "\n",
    "import nltk \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0c1ca0-a507-44a5-8538-7870b6fa350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aborcja = pd.read_csv('datasets/aborcja.csv', encoding='utf-8', sep=';', usecols=[\"statement\", \"score\"])\n",
    "dataset_armia_ue = pd.read_csv('datasets/armia_ue.csv', encoding='utf-8', sep=';')\n",
    "dataset_bron = pd.read_csv('datasets/bron.csv', encoding='utf-8', sep=';')\n",
    "dataset_cpk = pd.read_csv('datasets/cpk.csv', encoding='utf-8', sep=';')\n",
    "dataset_dochodowy = pd.read_csv('datasets/dochodowy.csv', encoding='utf-8', sep=';')\n",
    "dataset_euro = pd.read_csv('datasets/euro.csv', encoding='utf-8', sep=';')\n",
    "dataset_eutanazja = pd.read_csv('datasets/eutanazja.csv', encoding='utf-8', sep=';')\n",
    "dataset_imigranci= pd.read_csv('datasets/imigranci.csv', encoding='utf-8', sep=';')\n",
    "dataset_invitro = pd.read_csv('datasets/invitro.csv', encoding='utf-8', sep=';')\n",
    "dataset_kara_smierci = pd.read_csv('datasets/kara_smierci.csv', encoding='utf-8', sep=';')\n",
    "dataset_katastralny = pd.read_csv('datasets/katastralny.csv', encoding='utf-8', sep=';')\n",
    "dataset_osiemset = pd.read_csv('datasets/osiemset.csv', encoding='utf-8', sep=';')\n",
    "dataset_samochody = pd.read_csv('datasets/samochody.csv', encoding='utf-8', sep=';')\n",
    "dataset_sluzba_wojskowa = pd.read_csv('datasets/sluzba_wojskowa.csv', encoding='utf-8', sep=';')\n",
    "dataset_ue = pd.read_csv('datasets/ue.csv', encoding='utf-8', sep=';')\n",
    "dataset_wdowia= pd.read_csv('datasets/wdowia.csv', encoding='utf-8', sep=';')\n",
    "dataset_zus = pd.read_csv('datasets/zus.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "datasets = {\n",
    "    \"dataset_aborcja\": dataset_aborcja,\n",
    "    \"dataset_eutanazja\": dataset_eutanazja,\n",
    "    \"dataset_bron\": dataset_bron,\n",
    "    \"dataset_kara_smierci\": dataset_kara_smierci,\n",
    "    \"dataset_invitro\": dataset_invitro,\n",
    "    \"dataset_armia_ue\": dataset_armia_ue,\n",
    "    \"dataset_euro\": dataset_euro,\n",
    "    \"dataset_cpk\": dataset_cpk,\n",
    "    \"dataset_sluzba_wojskowa\": dataset_sluzba_wojskowa,\n",
    "    \"dataset_ue\": dataset_ue,\n",
    "    \"dataset_zus\": dataset_zus,\n",
    "    \"dataset_samochody\": dataset_samochody,\n",
    "    \"dataset_osiemset\": dataset_osiemset,\n",
    "    \"dataset_dochodowy\": dataset_dochodowy,\n",
    "    \"dataset_katastralny\": dataset_katastralny,\n",
    "    \"dataset_imigranci\": dataset_imigranci,\n",
    "    \"dataset_wdowia\": dataset_wdowia\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8e6411-a453-4adf-b9d4-02d60649d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statement     object\n",
       "score        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_aborcja.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dc961b-0113-40cc-97c0-d9c253a2762e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia dla dataset_aborcja: -0.00466321243523319\n",
      "Mediana dla dataset_aborcja: -0.1 \n",
      "\n",
      "Średnia dla dataset_eutanazja: 0.020967741935483883\n",
      "Mediana dla dataset_eutanazja: 0.0 \n",
      "\n",
      "Średnia dla dataset_bron: 0.038095238095238085\n",
      "Mediana dla dataset_bron: 0.1 \n",
      "\n",
      "Średnia dla dataset_kara_smierci: -0.09594594594594595\n",
      "Mediana dla dataset_kara_smierci: 0.0 \n",
      "\n",
      "Średnia dla dataset_invitro: 0.0\n",
      "Mediana dla dataset_invitro: -0.1 \n",
      "\n",
      "Średnia dla dataset_armia_ue: 0.09322033898305085\n",
      "Mediana dla dataset_armia_ue: 0.2 \n",
      "\n",
      "Średnia dla dataset_euro: 0.014655172413793128\n",
      "Mediana dla dataset_euro: -0.05 \n",
      "\n",
      "Średnia dla dataset_cpk: -0.033018867924528315\n",
      "Mediana dla dataset_cpk: -0.2 \n",
      "\n",
      "Średnia dla dataset_sluzba_wojskowa: -0.01666666666666667\n",
      "Mediana dla dataset_sluzba_wojskowa: 0.0 \n",
      "\n",
      "Średnia dla dataset_ue: -0.028571428571428564\n",
      "Mediana dla dataset_ue: 0.0 \n",
      "\n",
      "Średnia dla dataset_zus: 0.018518518518518517\n",
      "Mediana dla dataset_zus: 0.05 \n",
      "\n",
      "Średnia dla dataset_samochody: 0.023684210526315797\n",
      "Mediana dla dataset_samochody: 0.1 \n",
      "\n",
      "Średnia dla dataset_osiemset: -0.01720430107526883\n",
      "Mediana dla dataset_osiemset: -0.2 \n",
      "\n",
      "Średnia dla dataset_dochodowy: 0.07758620689655174\n",
      "Mediana dla dataset_dochodowy: -0.05 \n",
      "\n",
      "Średnia dla dataset_katastralny: -0.004166666666666684\n",
      "Mediana dla dataset_katastralny: -0.1 \n",
      "\n",
      "Średnia dla dataset_imigranci: -0.04220183486238532\n",
      "Mediana dla dataset_imigranci: -0.3 \n",
      "\n",
      "Średnia dla dataset_wdowia: -0.08514851485148517\n",
      "Mediana dla dataset_wdowia: -0.4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in datasets.items(): # iterowanie przez wszystkie pozycje klucz-wartość w słowniku. klucz to nazwy w cudzysłowie, a wartości to sam dataset.\n",
    "    print(f'Średnia dla {name}:', dataset['score'].mean())\n",
    "    print(f'Mediana dla {name}:', dataset['score'].median(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46373336-a0b6-4200-bfff-760ff8715048",
   "metadata": {},
   "source": [
    "## Wykonanie embeddingu wszystkich datasetów i zapis do plików .csv\n",
    "przeiterowanie po słowniku datasets, dokonanie embeddingów na każdym z nich oraz zapisanie odpowiednio oczyszczonych dataframeów do plików z rozszerzeniem .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b4649-31fa-42fa-a2fe-42b2bf39a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dla pojedynczego datasetu:\n",
    "dataset_embeddings = [\n",
    "    (get_embedding(row[\"statement\"]), row[\"score\"], row[\"statement\"]) \n",
    "    for _, row in dataset.iterrows() #embedding każdego datasetu\n",
    "]  \n",
    "df_dataset_embeddings = pd.DataFrame(dataset_embeddings, columns=[\"embedding\", \"score\", \"statement\"]) #konwersja zestawu danych do dataframe\n",
    "df_dataset_embeddings[\"embedding\"] = df_dataset_embeddings[\"embedding\"].apply(lambda x: \",\".join(map(str, x)))\n",
    "df_dataset_embeddings.to_csv(\"dataset_embeddings.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a963ea-8364-4e64-8dbb-c18daf1388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_embeddings_and_CSVsave():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset_embeddings = [\n",
    "            (get_embedding(model1, row[\"statement\"]), row[\"score\"], row[\"statement\"]) \n",
    "            for _, row in dataset.iterrows() #embedding każdego datasetu\n",
    "        ]\n",
    "        \n",
    "        df_dataset_embeddings = pd.DataFrame(dataset_embeddings, columns=[\"embedding\", \"score\", \"statement\"]) #konwersja zestawu danych do dataframe\n",
    "\n",
    "        df_dataset_embeddings[\"embedding\"] = df_dataset_embeddings[\"embedding\"].apply(lambda x: \",\".join(map(str, x)))\n",
    "        df_dataset_embeddings.to_csv(f\"{dataset_name}_embeddings.csv\", index=False, encoding=\"utf-8\") #zapis dataframe do pliku w formacie .csv\n",
    "        \n",
    "get_datasets_embeddings_and_CSVsave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8248c03-a021-4e38-8170-3c9400c0a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do konwersji tekstu na wektor (embedding)\n",
    "def get_embedding(model, text):\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f84683-d7ec-40bb-940a-450ee27bd53c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Załadowanie wszystkich zapisanych datasetów z plików .csv\n",
    "- załadowanie wszystkich zbiorów w formacie .csv z folderu \"datasets_embeddings\" i konwersja kolumny \"embedding\" na poprawną\n",
    "- wszystkie załadowane pliki są zapisane w zmiennych globalnych o nazwach: \"df_\"+nazwa datasetu np. df_dataset_aborcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b26f06-8e95-4d7a-8c0e-eec5c409591f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dla pojedynczego datasetu\n",
    "#df_loaded = pd.read_csv(\"datasets_embeddings/dataset_bron_embeddings.csv\", encoding=\"utf-8\")\n",
    "#df_loaded[\"embedding\"] = df_loaded[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "def CSVload_datasets_embedded(directory):\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        df = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "        df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "        dataset_name = f'df_{filename.replace(\"_embeddings.csv\", \"\")}'\n",
    "        \n",
    "        globals()[dataset_name] = df\n",
    "\n",
    "CSVload_datasets_embedded(\"datasets_embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e349bc-1eff-40c6-af63-28ecaf1a93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__session__', 'json', 'sys', '__import__', 'NamespaceMagics', '_jupyterlab_variableinspector_nms', '_jupyterlab_variableinspector_Jupyter', '__np', '__pd', '__pyspark', '__tf', '__K', '__torch', '__ipywidgets', '__xr', '_attempt_import', '_check_imported', '_jupyterlab_variableinspector_getsizeof', '_jupyterlab_variableinspector_getshapeof', '_jupyterlab_variableinspector_getcontentof', '_jupyterlab_variableinspector_is_matrix', '_jupyterlab_variableinspector_is_widget', '_jupyterlab_variableinspector_dict_list', '_jupyterlab_variableinspector_getmatrixcontent', '_jupyterlab_variableinspector_displaywidget', '_jupyterlab_variableinspector_default', '_jupyterlab_variableinspector_deletevariable', '_1', '_i', '_ii', '_iii', '_i1', 'SentenceTransformer', 'cosine_similarity', 'np', 'pd', 'plt', 'sns', 'nltk', 'SentimentIntensityAnalyzer', 'tqdm', 'AutoTokenizer', 'AutoModel', 'torch', 'time', 'os', '_2', '_i2', 'dataset_aborcja', 'dataset_eutanazja', 'dataset_bron', 'dataset_kara_smierci', 'dataset_invitro', 'dataset_armia_ue', 'dataset_euro', 'dataset_cpk', 'dataset_sluzba_wojskowa', 'dataset_ue', 'dataset_zus', 'dataset_samochody', 'dataset_osiemset', 'dataset_dochodowy', 'dataset_katastralny', 'dataset_imigranci', 'dataset_wdowia', 'datasets', '_3', '_i3', 'CSVload_datasets_embedded', 'df_dataset_aborcja', 'df_dataset_armia_ue', 'df_dataset_bron', 'df_dataset_cpk', 'df_dataset_dochodowy', 'df_dataset_euro', 'df_dataset_eutanazja', 'df_dataset_imigranci', 'df_dataset_invitro', 'df_dataset_kara_smierci', 'df_dataset_katastralny', 'df_dataset_osiemset', 'df_dataset_samochody', 'df_dataset_sluzba_wojskowa', 'df_dataset_ue', 'df_dataset_wdowia', 'df_dataset_zus', '_4', '_i4'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f84878f-3c8d-49e6-94c2-eac069a50238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\HuggingFaceCache\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"TRANSFORMERS_CACHE\"))  # Powinno zwrócić: D:\\huggingface_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d26a73-c049-45fc-a457-18075b919889",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb06ac4d-fa6b-4b37-a626-420721a6edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f813fe-966e-44bf-87b9-78c5fe9cd4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas ładowania modelu: 52.71 sekundy\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')\n",
    "#model = LongformerModel.from_pretrained('allenai/longformer-large-4096')\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model1 = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model2 = SentenceTransformer('BAAI/bge-m3') #nie\n",
    "model3 = SentenceTransformer('intfloat/multilingual-e5-small')\n",
    "model4 = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2') #nie\n",
    "model5 = SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')\n",
    "model6 = SentenceTransformer('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True) #nie\n",
    "model7 = SentenceTransformer('sdadas/st-polish-paraphrase-from-distilroberta')\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('BAAI/bge-multilingual-gemma2') 18gb\n",
    "#model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "#embedding = model1.encode(\"Przykładowe zdanie.\")\n",
    "#print(embedding.shape)  # ➜ (512,) albo (768,)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Czas ładowania modelu: {end_time - start_time:.2f} sekundy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4ae9657-34c8-4499-8be0-5e2cbc7f35b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "max_seq_length domyślnie: 128\n",
      "1024\n",
      "max_seq_length domyślnie: 8192\n",
      "384\n",
      "max_seq_length domyślnie: 512\n",
      "512\n",
      "max_seq_length domyślnie: 128\n",
      "768\n",
      "max_seq_length domyślnie: 128\n",
      "768\n",
      "max_seq_length domyślnie: 8192\n",
      "768\n",
      "max_seq_length domyślnie: 256\n"
     ]
    }
   ],
   "source": [
    "print(model1.get_sentence_embedding_dimension())\n",
    "print(\"max_seq_length domyślnie:\", model1.max_seq_length)\n",
    "print(model2.get_sentence_embedding_dimension())\n",
    "print(\"max_seq_length domyślnie:\", model2.max_seq_length)\n",
    "print(model3.get_sentence_embedding_dimension())\n",
    "print(\"max_seq_length domyślnie:\", model3.max_seq_length)\n",
    "print(model4.get_sentence_embedding_dimension())\n",
    "print(\"max_seq_length domyślnie:\", model4.max_seq_length)\n",
    "print(model5.get_sentence_embedding_dimension())\n",
    "print(\"max_seq_length domyślnie:\", model5.max_seq_length)\n",
    "print(model6.get_sentence_embedding_dimension())\n",
    "print(\"max_seq_length domyślnie:\", model6.max_seq_length)\n",
    "print(model7.get_sentence_embedding_dimension())\n",
    "print(\"max_seq_length domyślnie:\", model7.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b939f06-be16-4ae1-b3ff-926c9265de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/new-impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name1 = 'intfloat/multilingual-e5-small'\n",
    "model_name2 = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_name3 = 'sentence-transformers/stsb-xlm-r-multilingual'\n",
    "model_name4 = 'sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "model_name5 = 'Alibaba-NLP/gte-multilingual-base'\n",
    "model_name6 = 'BAAI/bge-m3'\n",
    "model_name7 = 'sdadas/st-polish-paraphrase-from-distilroberta'\n",
    "\n",
    "model_info1 = AutoModel.from_pretrained(model_name1, trust_remote_code=True)\n",
    "model_info2 = AutoModel.from_pretrained(model_name2, trust_remote_code=True)\n",
    "model_info3 = AutoModel.from_pretrained(model_name3, trust_remote_code=True)\n",
    "model_info4 = AutoModel.from_pretrained(model_name4, trust_remote_code=True)\n",
    "model_info5 = AutoModel.from_pretrained(model_name5, trust_remote_code=True)\n",
    "model_info6 = AutoModel.from_pretrained(model_name6, trust_remote_code=True)\n",
    "model_info7 = AutoModel.from_pretrained(model_name7, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e581f1-14f5-4779-84e1-1e717cbbedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model_info):\n",
    "    print(f'---------{model_info.name_or_path}-----------')\n",
    "    print(model_info.config)\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6a9d1a8-0ff9-4f5c-86db-5b08caccea4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------intfloat/multilingual-e5-small-----------\n",
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"XLMRobertaTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "-------------------------------\n",
      "---------sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2-----------\n",
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250037\n",
      "}\n",
      "\n",
      "-------------------------------\n",
      "---------sentence-transformers/stsb-xlm-r-multilingual-----------\n",
      "XLMRobertaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "-------------------------------\n",
      "---------sentence-transformers/distiluse-base-multilingual-cased-v2-----------\n",
      "DistilBertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "-------------------------------\n",
      "---------Alibaba-NLP/gte-multilingual-base-----------\n",
      "NewConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"NewModel\",\n",
      "    \"NewForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"Alibaba-NLP/new-impl--configuration.NewConfig\",\n",
      "    \"AutoModel\": \"Alibaba-NLP/new-impl--modeling.NewModel\",\n",
      "    \"AutoModelForMaskedLM\": \"Alibaba-NLP/new-impl--modeling.NewForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"Alibaba-NLP/new-impl--modeling.NewForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"Alibaba-NLP/new-impl--modeling.NewForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"Alibaba-NLP/new-impl--modeling.NewForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"Alibaba-NLP/new-impl--modeling.NewForTokenClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_norm_type\": \"layer_norm\",\n",
      "  \"logn_attention_clip1\": false,\n",
      "  \"logn_attention_scale\": false,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"new\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pack_qkv\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rope\",\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"type\": \"ntk\"\n",
      "  },\n",
      "  \"rope_theta\": 20000,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"unpad_inputs\": false,\n",
      "  \"use_memory_efficient_attention\": false,\n",
      "  \"vocab_size\": 250048\n",
      "}\n",
      "\n",
      "-------------------------------\n",
      "---------BAAI/bge-m3-----------\n",
      "XLMRobertaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "-------------------------------\n",
      "---------sdadas/st-polish-paraphrase-from-distilroberta-----------\n",
      "RobertaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50001\n",
      "}\n",
      "\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_model_info(model_info1)\n",
    "get_model_info(model_info2)\n",
    "get_model_info(model_info3)\n",
    "get_model_info(model_info4)\n",
    "get_model_info(model_info5)\n",
    "get_model_info(model_info6)\n",
    "get_model_info(model_info7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e99b69dd-2d1a-4e51-90e2-5efcc713d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_texts_similarity(model_to_use, text1, text2):\n",
    "    embedding1 = model_to_use.encode(text1)\n",
    "    embedding2 = model_to_use.encode(text2)\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "\n",
    "    print(f\"Podobieństwo dla {model_to_use._first_module().auto_model.config._name_or_path}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea470426-9154-4709-a7a0-114427493330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aborcja niepodobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.6233\n",
      "Podobieństwo dla BAAI/bge-m3: 0.6353\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.9017\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.5515\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.5147\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.6469\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 0.5306\n",
      "Aborcja podobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.7167\n",
      "Podobieństwo dla BAAI/bge-m3: 0.6903\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.9000\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.5632\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.6804\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.7303\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 0.6099\n",
      "\n",
      "800+ niepodobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.4875\n",
      "Podobieństwo dla BAAI/bge-m3: 0.5731\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.9247\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.5241\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.4100\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.6451\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 1.0000\n",
      "800+ podobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.5516\n",
      "Podobieństwo dla BAAI/bge-m3: 0.7337\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.9174\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.4742\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.7227\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.6860\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 0.4879\n",
      "\n",
      "Polityka co do UE niepodobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.6011\n",
      "Podobieństwo dla BAAI/bge-m3: 0.5758\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.8927\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.4810\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.5639\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.5653\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 0.5995\n",
      "Polityka co do UE podobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.6360\n",
      "Podobieństwo dla BAAI/bge-m3: 0.6188\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.8900\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.3184\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.6546\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.5993\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 0.6699\n",
      "\n",
      "Eutanazja niepodobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.6536\n",
      "Podobieństwo dla BAAI/bge-m3: 0.6533\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.9330\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.4821\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.6117\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.7248\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 0.5954\n",
      "Eutanazja podobne:\n",
      "Podobieństwo dla sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 0.7383\n",
      "Podobieństwo dla BAAI/bge-m3: 0.7461\n",
      "Podobieństwo dla intfloat/multilingual-e5-small: 0.9286\n",
      "Podobieństwo dla sentence-transformers/distiluse-base-multilingual-cased-v2: 0.4260\n",
      "Podobieństwo dla sentence-transformers/stsb-xlm-r-multilingual: 0.7657\n",
      "Podobieństwo dla Alibaba-NLP/gte-multilingual-base: 0.7108\n",
      "Podobieństwo dla sdadas/st-polish-paraphrase-from-distilroberta: 0.7801\n"
     ]
    }
   ],
   "source": [
    "aborcja1 = \"Każde życie zaczyna się od poczęcia i każda aborcja to morderstwo, niezależnie od okoliczności – nikt nie ma prawa odbierać życia niewinnemu dziecku.\"\n",
    "aborcja2 = \"Aborcja była, jest i będzie. Można ją robić legalnie i bezpiecznie albo w podziemiu, gdzie kobiety narażają swoje zdrowie i życie. Wybór jest prosty.\"\n",
    "aborcja3 = \"Każda kobieta ma prawo do swojego życia i swoich marzeń. Jeśli nie chce mieć dziecka, to jej sprawa, a społeczeństwo nie ma prawa jej do tego zmuszać.\"\n",
    "aborcja4 = \"Zmuszanie kobiet do rodzenia niechcianych dzieci jest nieludzkie i skazuje je na życie w biedzie i cierpieniu. To powinien być wybór, a nie kara za zajście w ciążę.\"\n",
    "osiemset1 = \"Nie ma co ukrywać, program 800+ pomógł wielu rodzinom, zwłaszcza tym, które naprawdę tego potrzebowały.\"\n",
    "osiemset2 = \"800+ to tylko obietnice rządu, które nie zmieniły realnych problemów rodzinnych w Polsce. To nie rozwiązuje naszych problemów.\"\n",
    "osiemset3 = \"Zamiast 800+, lepiej zainwestować w programy, które umożliwią ludziom wyjście z ubóstwa, a nie tylko dawać pieniądze na chwilę.\"\n",
    "osiemset4 = \"Program 800+ to rozwiązanie, które zaspokaja tylko część potrzeb, a nie zmienia strukturalnych problemów w społeczeństwie.\"\n",
    "ue1 = \"Unia Europejska mówi nam, co mamy robić i ogranicza naszą niezależność. To tak, jakby ktoś inny rządził naszym krajem.\"\n",
    "ue2 = \"Dzięki UE mamy swobodę podróżowania, pracy i nauki w całej Europie. Wyjście byłoby katastrofą dla młodego pokolenia.\"\n",
    "ue3 = \"Unia Europejska to gwarancja pokoju i stabilności. Bez niej Polska byłaby samotną wyspą wśród potężnych sąsiadów.\"\n",
    "ue4 = \"Europa przez wieki była targana konfliktami. Unia Europejska, mimo wszystkich problemów, to najlepszy model współpracy międzynarodowej, jaki kiedykolwiek stworzyliśmy.\"\n",
    "eutanazja1 = \"Eutanazja w żadnym przypadku nie powinna mieć miejsca, ponieważ życie ludzkie ma nieocenioną wartość.\"\n",
    "eutanazja2 = \"Eutanazja powinna być dostępna dla wszystkich, którzy doświadczają nieuleczalnego cierpienia, niezależnie od przyczyn.\"\n",
    "eutanazja3 = \"Eutanazja jest moralnie nieakceptowalna i absolutnie nie powinna mieć miejsca w żadnym społeczeństwie.\"\n",
    "eutanazja4 = \"Eutanazja to zło, które powinno być zakazane w każdym przypadku, nawet w przypadku nieuleczalnych chorób.\"\n",
    "\n",
    "print(\"Aborcja niepodobne:\")\n",
    "get_two_texts_similarity(model1, aborcja1, aborcja2)\n",
    "get_two_texts_similarity(model2, aborcja1, aborcja2)\n",
    "get_two_texts_similarity(model3, aborcja1, aborcja2)\n",
    "get_two_texts_similarity(model4, aborcja1, aborcja2)\n",
    "get_two_texts_similarity(model5, aborcja1, aborcja2)\n",
    "get_two_texts_similarity(model6, aborcja1, aborcja2)\n",
    "get_two_texts_similarity(model7, aborcja1, aborcja2)\n",
    "print(\"Aborcja podobne:\")\n",
    "get_two_texts_similarity(model1, aborcja3, aborcja4)\n",
    "get_two_texts_similarity(model2, aborcja3, aborcja4)\n",
    "get_two_texts_similarity(model3, aborcja3, aborcja4)\n",
    "get_two_texts_similarity(model4, aborcja3, aborcja4)\n",
    "get_two_texts_similarity(model5, aborcja3, aborcja4)\n",
    "get_two_texts_similarity(model6, aborcja3, aborcja4)\n",
    "get_two_texts_similarity(model7, aborcja3, aborcja4)\n",
    "print(\"\\n800+ niepodobne:\")\n",
    "get_two_texts_similarity(model1, osiemset1, osiemset2)\n",
    "get_two_texts_similarity(model2, osiemset1, osiemset2)\n",
    "get_two_texts_similarity(model3, osiemset1, osiemset2)\n",
    "get_two_texts_similarity(model4, osiemset1, osiemset2)\n",
    "get_two_texts_similarity(model5, osiemset1, osiemset2)\n",
    "get_two_texts_similarity(model6, osiemset1, osiemset2)\n",
    "get_two_texts_similarity(model7, osiemset2, osiemset2)\n",
    "print(\"800+ podobne:\")\n",
    "get_two_texts_similarity(model1, osiemset3, osiemset4)\n",
    "get_two_texts_similarity(model2, osiemset3, osiemset4)\n",
    "get_two_texts_similarity(model3, osiemset3, osiemset4)\n",
    "get_two_texts_similarity(model4, osiemset3, osiemset4)\n",
    "get_two_texts_similarity(model5, osiemset3, osiemset4)\n",
    "get_two_texts_similarity(model6, osiemset3, osiemset4)\n",
    "get_two_texts_similarity(model7, osiemset3, osiemset4)\n",
    "print(\"\\nPolityka co do UE niepodobne:\")\n",
    "get_two_texts_similarity(model1, ue1, ue2)\n",
    "get_two_texts_similarity(model2, ue1, ue2)\n",
    "get_two_texts_similarity(model3, ue1, ue2)\n",
    "get_two_texts_similarity(model4, ue1, ue2)\n",
    "get_two_texts_similarity(model5, ue1, ue2)\n",
    "get_two_texts_similarity(model6, ue1, ue2)\n",
    "get_two_texts_similarity(model7, ue1, ue2)\n",
    "print(\"Polityka co do UE podobne:\")\n",
    "get_two_texts_similarity(model1, ue3, ue4)\n",
    "get_two_texts_similarity(model2, ue3, ue4)\n",
    "get_two_texts_similarity(model3, ue3, ue4)\n",
    "get_two_texts_similarity(model4, ue3, ue4)\n",
    "get_two_texts_similarity(model5, ue3, ue4)\n",
    "get_two_texts_similarity(model6, ue3, ue4)\n",
    "get_two_texts_similarity(model7, ue3, ue4)\n",
    "print(\"\\nEutanazja niepodobne:\")\n",
    "get_two_texts_similarity(model1, eutanazja1, eutanazja2)\n",
    "get_two_texts_similarity(model2, eutanazja1, eutanazja2)\n",
    "get_two_texts_similarity(model3, eutanazja1, eutanazja2)\n",
    "get_two_texts_similarity(model4, eutanazja1, eutanazja2)\n",
    "get_two_texts_similarity(model5, eutanazja1, eutanazja2)\n",
    "get_two_texts_similarity(model6, eutanazja1, eutanazja2)\n",
    "get_two_texts_similarity(model7, eutanazja1, eutanazja2)\n",
    "print(\"Eutanazja podobne:\")\n",
    "get_two_texts_similarity(model1, eutanazja3, eutanazja4)\n",
    "get_two_texts_similarity(model2, eutanazja3, eutanazja4)\n",
    "get_two_texts_similarity(model3, eutanazja3, eutanazja4)\n",
    "get_two_texts_similarity(model4, eutanazja3, eutanazja4)\n",
    "get_two_texts_similarity(model5, eutanazja3, eutanazja4)\n",
    "get_two_texts_similarity(model6, eutanazja3, eutanazja4)\n",
    "get_two_texts_similarity(model7, eutanazja3, eutanazja4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db73148-1d46-46e7-9158-9f7ba187398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(user_input, df_dataset):\n",
    "    # Wektor wypowiedzi użytkownika\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    \n",
    "    # Oblicz podobieństwo dla każdej wypowiedzi w DataFrame\n",
    "    df_dataset[\"similarity\"] = df_dataset[\"embedding\"].apply(lambda emb: cosine_similarity([user_embedding], [emb])[0][0])\n",
    "    # Posortuj według podobieństwa malejąco\n",
    "    df_sorted = df_dataset.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    top_n = 5  # Weź 5 najbliższych wypowiedzi\n",
    "    top_similar = df_sorted.head(top_n)\n",
    "\n",
    "    if top_similar.iloc[0][\"similarity\"] > 0.9:\n",
    "        avg_score = top_similar.iloc[0][\"score\"]\n",
    "    else:\n",
    "        avg_score = top_similar[\"score\"].mean()\n",
    "    return avg_score, top_similar[[\"similarity\", \"score\", \"statement\"]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f2a657-d45a-482d-8cce-85e10b945e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_upgraded(user_input, df_dataset):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    \n",
    "    df_dataset[\"similarity\"] = df_dataset[\"embedding\"].apply(lambda emb: cosine_similarity([user_embedding], [emb])[0][0])\n",
    "    df_sorted = df_dataset.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    top_n = 5\n",
    "    top_similar = df_sorted.head(top_n)\n",
    "\n",
    "    if top_similar.iloc[0][\"similarity\"] > 0.9:\n",
    "        avg_score = top_similar.iloc[0][\"score\"]\n",
    "    else:\n",
    "        avg_score = top_similar[\"score\"].mean()\n",
    "        lower_bound = max(avg_score - 0.8, -1.0)\n",
    "        upper_bound = min(avg_score + 0.8, 1.0)\n",
    "\n",
    "        df_dataset_filtered = df_dataset[(df_dataset[\"score\"] >= lower_bound) & (df_dataset[\"score\"] <= upper_bound)]\n",
    "        df_dataset_filtered = df_dataset_filtered.sort_values(by=\"similarity\", ascending=False)\n",
    "        top_similar = df_dataset_filtered.head(top_n)\n",
    "        avg_score = top_similar[\"score\"].mean()\n",
    "        \n",
    "    return avg_score, top_similar[[\"similarity\", \"score\", \"statement\"]].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68fddaa9-064f-4db6-bf14-b3b6d5b667c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_scaling\u001b[39m(x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mcopysign(math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(x)), x)\n\u001b[1;32m---> 12\u001b[0m tanh_result \u001b[38;5;241m=\u001b[39m tanh_scaling(predicted_score)\n\u001b[0;32m     13\u001b[0m power_result \u001b[38;5;241m=\u001b[39m power_scaling(predicted_score)\n\u001b[0;32m     14\u001b[0m log_result \u001b[38;5;241m=\u001b[39m log_scaling(predicted_score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_score' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def tanh_scaling(x, k=1.4):\n",
    "    return math.tanh(k * x)\n",
    "\n",
    "def power_scaling(x, b=0.5):\n",
    "    return math.copysign(abs(x) ** b, x)\n",
    "\n",
    "def log_scaling(x):\n",
    "    return math.copysign(math.log(1 + abs(x)), x)\n",
    "\n",
    "tanh_result = tanh_scaling(predicted_score)\n",
    "power_result = power_scaling(predicted_score)\n",
    "log_result = log_scaling(predicted_score)\n",
    "\n",
    "print(f\"Original value: {predicted_score}\")\n",
    "print(f\"Tanh scaling: {tanh_result}\")\n",
    "print(f\"Power scaling: {power_result}\")\n",
    "print(f\"Log scaling: {log_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f14ae-5d14-4dc3-86a0-1925ec5939d9",
   "metadata": {},
   "source": [
    "# Funkcjonalność kompasu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b9be4c-b75d-409a-a0ef-6159c5d98dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacyfizm_militaryzm = ['bron', 'obronnosc', 'sluzba_wojskowa']\n",
    "nacjonalizm_kosmopolityzm = ['obronnosc', 'sluzba_wojskowa', 'armia_ue', 'euro', 'cpk', 'ue', 'imigranci']\n",
    "ekologia_industrializm = ['samochody', 'cpk']\n",
    "eurofederalizm_eurosceptyzm = ['euro', 'armia_ue', 'ue', 'samochody']\n",
    "progresywizm_tradycjonalizm = ['aborcja', 'eutanazja', 'invitro', 'kara_smierci', 'bron']\n",
    "socjalizm_liberalizm = ['osiemset', 'zus', 'dochodowy', 'katastralny', 'wdowia']\n",
    "regulacjonizm_leseferyzm =  ['osiemset', 'zus', 'dochodowy', 'katastralny', 'wdowia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8216660d-fef6-4e8a-9c8e-abe58aae6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self):\n",
    "        self.pacyfizm_militaryzm_score = 0\n",
    "        self.pacyfizm_militaryzm_answers = 0\n",
    "        \n",
    "        self.nacjonalizm_kosmopolityzm_score = 0\n",
    "        self.nacjonalizm_kosmopolityzm_answers = 0\n",
    "\n",
    "        self.ekologia_industrializm_score = 0\n",
    "        self.ekologia_industrializm_answers = 0\n",
    "\n",
    "        self.eurofederalizm_eurosceptyzm_score = 0        \n",
    "        self.eurofederalizm_eurosceptyzm_answers = 0\n",
    "\n",
    "        self.progresywizm_tradycjonalizm_score = 0        \n",
    "        self.progresywizm_tradycjonalizm_answers = 0\n",
    "\n",
    "        self.socjalizm_liberalizm_score = 0        \n",
    "        self.socjalizm_liberalizm_answers = 0\n",
    "\n",
    "        self.regulacjonizm_leseferyzm_score = 0        \n",
    "        self.regulacjonizm_leseferyzm_answers = 0\n",
    "\n",
    "        self.nolan_gospodarka_score = 0\n",
    "        self.nolan_gospodarka_answers = 0\n",
    "\n",
    "        self.nolan_obyczajowe_score = 0\n",
    "        self.nolan_obyczajowe_answers = 0\n",
    "\n",
    "    def add_score_to_compass(self, user_statement, chosen_topic, chosen_dataset):\n",
    "        user_score, similiar_results = get_similarity_upgraded(user_statement, chosen_dataset)\n",
    "\n",
    "        if chosen_topic in socjalizm_liberalizm or chosen_topic in regulacjonizm_leseferyzm:\n",
    "            if chosen_topic in socjalizm_liberalizm:\n",
    "                self.socjalizm_liberalizm_score += user_score\n",
    "                self.socjalizm_liberalizm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in regulacjonizm_leseferyzm:\n",
    "                self.regulacjonizm_leseferyzm_score += user_score\n",
    "                self.regulacjonizm_leseferyzm_answers += 1   \n",
    "                \n",
    "            self.nolan_gospodarka_score += user_score\n",
    "            self.nolan_gospodarka_answers += 1\n",
    "            \n",
    "        else:  \n",
    "            if chosen_topic in pacyfizm_militaryzm:       \n",
    "                self.pacyfizm_militaryzm_score += user_score\n",
    "                self.pacyfizm_militaryzm_answers += 1   \n",
    "    \n",
    "            if chosen_topic in nacjonalizm_kosmopolityzm:        \n",
    "                self.nacjonalizm_kosmopolityzm_score += user_score\n",
    "                self.nacjonalizm_kosmopolityzm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in ekologia_industrializm:        \n",
    "                self.ekologia_industrializm_score += user_score\n",
    "                self.ekologia_industrializm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in eurofederalizm_eurosceptyzm:       \n",
    "                self.eurofederalizm_eurosceptyzm_score += user_score\n",
    "                self.eurofederalizm_eurosceptyzm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in progresywizm_tradycjonalizm:\n",
    "                self.progresywizm_tradycjonalizm_score += user_score\n",
    "                self.progresywizm_tradycjonalizm_answers += 1\n",
    "                \n",
    "            self.nolan_obyczajowe_score += user_score\n",
    "            self.nolan_obyczajowe_answers += 1\n",
    "\n",
    "\n",
    "    def display_scores(self):\n",
    "        if self.pacyfizm_militaryzm_answers != 0:\n",
    "            print(f'Wartość dla pacyfizm-militaryzm wynosi {self.pacyfizm_militaryzm_score/self.pacyfizm_militaryzm_answers}')\n",
    "        if self.nacjonalizm_kosmopolityzm_answers != 0:\n",
    "            print(f'Wartość dla nacjonalizm-kosmopolityzm wynosi {self.nacjonalizm_kosmopolityzm_score/self.nacjonalizm_kosmopolityzm_answers}')\n",
    "        if self.ekologia_industrializm_answers != 0:\n",
    "            print(f'Wartość dla ekologia-industrializm wynosi {self.ekologia_industrializm_score/self.ekologia_industrializm_answers}')\n",
    "        if self.eurofederalizm_eurosceptyzm_answers != 0:\n",
    "            print(f'Wartość dla eurofederalizm-eurosceptyzm wynosi {self.eurofederalizm_eurosceptyzm_score/self.eurofederalizm_eurosceptyzm_answers}')\n",
    "        if self.progresywizm_tradycjonalizm_answers != 0:\n",
    "            print(f'Wartość dla progresywizm-tradycjonalizm wynosi {self.progresywizm_tradycjonalizm_score/self.progresywizm_tradycjonalizm_answers}')\n",
    "        if self.socjalizm_liberalizm_answers != 0:\n",
    "            print(f'Wartość dla socjalizm-liberalizm wynosi {self.socjalizm_liberalizm_score/self.socjalizm_liberalizm_answers}')\n",
    "        if self.regulacjonizm_leseferyzm_answers != 0:\n",
    "            print(f'Wartość dla regulacjonizm-leseferyzm wynosi {self.regulacjonizm_leseferyzm_score/self.regulacjonizm_leseferyzm_answers}')\n",
    "        \n",
    "        if self.nolan_obyczajowe_answers != 0:\n",
    "            print(f'Wartość dla diagramu Nolana konserwatyzm-liberalizm wynosi {self.nolan_obyczajowe_score/self.nolan_obyczajowe_answers}')\n",
    "        if self.nolan_gospodarka_answers != 0:\n",
    "            print(f'Wartość dla diagramu Nolana socjalizm-wolny rynek wynosi {self.nolan_gospodarka_score/self.nolan_gospodarka_answers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8acec7cd-5407-4564-92fd-a3e956e7400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = User()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3f0bdbe-f474-4636-a2aa-4ee6d4fd2577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aborcja\n",
      "armia_ue\n",
      "bron\n",
      "cpk\n",
      "dochodowy\n",
      "euro\n",
      "eutanazja\n",
      "imigranci\n",
      "invitro\n",
      "kara_smierci\n",
      "katastralny\n",
      "osiemset\n",
      "samochody\n",
      "sluzba_wojskowa\n",
      "ue\n",
      "wdowia\n",
      "zus\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"datasets_embeddings\"):\n",
    "    filepath = os.path.join(\"datasets_embeddings\", filename)\n",
    "\n",
    "    topic = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    topic[\"embedding\"] = topic[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "    topic_name = f'{filename.replace(\"_embeddings.csv\", \"\").replace(\"dataset_\", \"\")}'\n",
    "    print(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef23b831-9985-4181-a6e9-9427df03cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " bron\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset o nazwie df_dataset_bron ZNALEZIONY.\n"
     ]
    }
   ],
   "source": [
    "chosen_topic = input()\n",
    "chosen_dataset_name = f'df_dataset_{chosen_topic}'\n",
    "if chosen_dataset_name in globals():\n",
    "    chosen_dataset = globals()[chosen_dataset_name]\n",
    "    print(f\"Dataset o nazwie {chosen_dataset_name} ZNALEZIONY.\")\n",
    "else:\n",
    "    print(f\"Dataset o nazwie {chosen_dataset_name} NIE ISTNIEJE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99a31024-a07d-4387-80d4-9a37004b57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Ogólnie jestem za łatwiejszym dostępem do posiadania broni palnej, bo to oznacza większe bezpieczeństwo jednostki, ale nie chciałbym mieć u nas drugiego USA. Restrykcje i testy powinny dalej istnieć.\n"
     ]
    }
   ],
   "source": [
    "user_statement = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dbdd5dfc-2a95-4704-8bbb-cf29bfda3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user.add_score_to_compass(user_statement, chosen_topic, chosen_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c267ed1-a6a9-4afa-9085-051b6b36753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartość dla pacyfizm-militaryzm wynosi 0.24000000000000002\n",
      "Wartość dla nacjonalizm-kosmopolityzm wynosi 0.32\n",
      "Wartość dla ekologia-industrializm wynosi 0.73\n",
      "Wartość dla eurofederalizm-eurosceptyzm wynosi 0.34\n",
      "Wartość dla progresywizm-tradycjonalizm wynosi 0.328\n",
      "Wartość dla socjalizm-liberalizm wynosi 0.26\n",
      "Wartość dla regulacjonizm-leseferyzm wynosi 0.26\n",
      "Wartość dla diagramu Nolana konserwatyzm-liberalizm wynosi 0.35999999999999993\n",
      "Wartość dla diagramu Nolana socjalizm-wolny rynek wynosi 0.26\n"
     ]
    }
   ],
   "source": [
    "user.display_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "393c6982-902e-417f-baf9-f5c21891e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni score: 0.34\n",
      "Najbardziej podobne wypowiedzi:\n",
      "Similarity: 0.7392, Score: 0.7, Text: Jestem za posiadaniem broni, ale pod warunkiem, że osoby ją kupujące przechodzą dokładne testy psychologiczne i odpowiednie szkolenia. Nie można dopuścić do sytuacji, w której ktoś niestabilny dostaje broń.\n",
      "Similarity: 0.6888, Score: 0.1, Text: Nie uważam, żeby całkowity zakaz był konieczny, ale dostęp do broni powinien być dużo trudniejszy.\n",
      "Similarity: 0.6840, Score: 0.1, Text: Obrona własna to jedno, ale broń nie powinna być zbyt łatwo dostępna.\n",
      "Similarity: 0.6821, Score: 0.3, Text: Nie jestem wielkim fanem broni, ale rozumiem ludzi, którzy chcą jej mieć dla bezpieczeństwa.\n",
      "Similarity: 0.6644, Score: 0.5, Text: Posiadanie broni powinno być przywilejem, a nie powszechnym prawem.\n"
     ]
    }
   ],
   "source": [
    "avg_score, results = get_similarity(user_statement, chosen_dataset)\n",
    "\n",
    "print(\"Średni score:\", round(avg_score, 3))\n",
    "print(\"Najbardziej podobne wypowiedzi:\")\n",
    "for sim, score, text in results:\n",
    "    print(f\"Similarity: {sim:.4f}, Score: {score}, Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ced0c79b-d152-4734-9865-c5bf6b902023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ogólnie jestem za łatwiejszym dostępem do posiadania broni palnej, bo to oznacza większe bezpieczeństwo jednostki, ale nie chciałbym mieć u nas drugiego USA. Restrykcje i testy powinny dalej istnieć.\n",
      "\n",
      "\n",
      "Średni score: 0.34\n",
      "Najbardziej podobne wypowiedzi:\n",
      "\n",
      "Similarity: 0.7392, Score: 0.7, Text: Jestem za posiadaniem broni, ale pod warunkiem, że osoby ją kupujące przechodzą dokładne testy psychologiczne i odpowiednie szkolenia. Nie można dopuścić do sytuacji, w której ktoś niestabilny dostaje broń.\n",
      "\n",
      "Similarity: 0.6888, Score: 0.1, Text: Nie uważam, żeby całkowity zakaz był konieczny, ale dostęp do broni powinien być dużo trudniejszy.\n",
      "\n",
      "Similarity: 0.6840, Score: 0.1, Text: Obrona własna to jedno, ale broń nie powinna być zbyt łatwo dostępna.\n",
      "\n",
      "Similarity: 0.6821, Score: 0.3, Text: Nie jestem wielkim fanem broni, ale rozumiem ludzi, którzy chcą jej mieć dla bezpieczeństwa.\n",
      "\n",
      "Similarity: 0.6644, Score: 0.5, Text: Posiadanie broni powinno być przywilejem, a nie powszechnym prawem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_score, results = get_similarity_upgraded(user_statement, chosen_dataset)\n",
    "\n",
    "print(user_statement)\n",
    "print('\\n')\n",
    "print(\"Średni score:\", round(avg_score, 3))\n",
    "print(\"Najbardziej podobne wypowiedzi:\\n\")\n",
    "for sim, score, text in results:\n",
    "    print(f\"Similarity: {sim:.4f}, Score: {score}, Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "315468d9-81a6-4e62-87ef-671c2aee1e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value: 0.1\n",
      "Tanh scaling: 0.139092447878458\n",
      "Power scaling: 0.31622776601683794\n",
      "Log scaling: 0.09531017980432493\n",
      "Aplify scaling: 0.1308675937775247\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "avg_score= 0.1\n",
    "\n",
    "def tanh_scaling(x, k=1.4):\n",
    "    return math.tanh(k * x)\n",
    "\n",
    "def power_scaling(x, b=0.5):\n",
    "    return math.copysign(abs(x) ** b, x)\n",
    "\n",
    "def log_scaling(x):\n",
    "    return math.copysign(math.log(1 + abs(x)), x)\n",
    "    \n",
    "def amplify(x, scale=1):\n",
    "    return math.tanh(x * scale) / math.tanh(scale)\n",
    "\n",
    "\n",
    "tanh_result = tanh_scaling(avg_score)\n",
    "power_result = power_scaling(avg_score)\n",
    "log_result = log_scaling(avg_score)\n",
    "amplify_result = amplify(avg_score)\n",
    "\n",
    "\n",
    "print(f\"Original value: {avg_score}\")\n",
    "print(f\"Tanh scaling: {tanh_result}\")\n",
    "print(f\"Power scaling: {power_result}\")\n",
    "print(f\"Log scaling: {log_result}\")\n",
    "print(f\"Aplify scaling: {amplify_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e4e1aaf-ee59-4c2f-8e09-35d04a03ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla 0.0 wzmocnienie wynosi 0.00, wzrost o 0.00\n",
      "Dla 0.05 wzmocnienie wynosi 0.07, wzrost o 0.02\n",
      "Dla 0.1 wzmocnienie wynosi 0.13, wzrost o 0.03\n",
      "Dla 0.15000000000000002 wzmocnienie wynosi 0.20, wzrost o 0.05\n",
      "Dla 0.2 wzmocnienie wynosi 0.26, wzrost o 0.06\n",
      "Dla 0.25 wzmocnienie wynosi 0.32, wzrost o 0.07\n",
      "Dla 0.30000000000000004 wzmocnienie wynosi 0.38, wzrost o 0.08\n",
      "Dla 0.35000000000000003 wzmocnienie wynosi 0.44, wzrost o 0.09\n",
      "Dla 0.4 wzmocnienie wynosi 0.50, wzrost o 0.10\n",
      "Dla 0.45 wzmocnienie wynosi 0.55, wzrost o 0.10\n",
      "Dla 0.5 wzmocnienie wynosi 0.61, wzrost o 0.11\n",
      "Dla 0.55 wzmocnienie wynosi 0.66, wzrost o 0.11\n",
      "Dla 0.6000000000000001 wzmocnienie wynosi 0.71, wzrost o 0.11\n",
      "Dla 0.65 wzmocnienie wynosi 0.75, wzrost o 0.10\n",
      "Dla 0.7000000000000001 wzmocnienie wynosi 0.79, wzrost o 0.09\n",
      "Dla 0.75 wzmocnienie wynosi 0.83, wzrost o 0.08\n",
      "Dla 0.8 wzmocnienie wynosi 0.87, wzrost o 0.07\n",
      "Dla 0.8500000000000001 wzmocnienie wynosi 0.91, wzrost o 0.06\n",
      "Dla 0.9 wzmocnienie wynosi 0.94, wzrost o 0.04\n",
      "Dla 0.9500000000000001 wzmocnienie wynosi 0.97, wzrost o 0.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.arange(0, 1, 0.05):\n",
    "    print(f'Dla {i} wzmocnienie wynosi {amplify(i):.2f}, wzrost o {(amplify(i)-i):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb486a-6afd-4fb8-8cc5-03f9b6d50d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7617548-736f-453e-a2c5-294f40386328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "918822bc-766b-4741-a2dd-7c633c59ff93",
   "metadata": {},
   "source": [
    "## PolBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9b125e-1238-426c-a747-25fdbc951695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import torch\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "\n",
    "word_embedding_model = models.Transformer(\n",
    "    'dkleczek/bert-base-polish-cased-v1',\n",
    "    max_seq_length=128\n",
    ")\n",
    "\n",
    "# Krok 2: Dodaj warstwę pooling (np. mean pooling po tokenach)\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False\n",
    ")\n",
    "\n",
    "# Krok 3: Zbuduj SentenceTransformer z tych komponentów\n",
    "model_polbert = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bbebfe-59bb-4e3f-b1e8-1c7c300bacab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./polbert-fine-tune/PolBERT_trained2 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "word_embedding_model = models.Transformer(\n",
    "    './polbert-fine-tune/PolBERT_trained2',  # <- Ścieżka do Twojego folderu z modelem\n",
    "    max_seq_length=128\n",
    ")\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False\n",
    ")\n",
    "\n",
    "# Krok 3: Złożenie modelu SentenceTransformer\n",
    "model_polsejm = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc4a1fa-c50d-42f9-aa4c-24e36e335e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oryginalny PolBERT\n",
      "Podobieństwo: 0.6917\n",
      "Podobieństwo: 0.6873\n",
      "Sejmowy PolBERT\n",
      "Podobieństwo: 0.6881\n",
      "Podobieństwo: 0.6607\n"
     ]
    }
   ],
   "source": [
    "s1 = \"Aborcja to ludobójstwo na bezbronnych. Powinniśmy chronić życie od poczęcia do naturalnej śmierci.\"\n",
    "s2 = \"Każda aborcja to morderstwo, bez względu na okoliczności. Nie ma wyjątków, nie ma dyskusji. Nie można legalizować zabijania.\"\n",
    "s3 = \"Aborcja powinna być zabroniona, a kobiety, które zdecydują się na jej przeprowadzenie, powinny być karane.\"\n",
    "s4 = \"Każda kobieta ma prawo do swojego życia i swoich marzeń. Jeśli nie chce mieć dziecka, to jej sprawa, a społeczeństwo nie ma prawa jej do tego zmuszać.\"\n",
    "\n",
    "print(\"Oryginalny PolBERT\")\n",
    "emb1 = model_polbert.encode(s1, convert_to_tensor=True)\n",
    "emb2 = model_polbert.encode(s2, convert_to_tensor=True)\n",
    "emb3 = model_polbert.encode(s3, convert_to_tensor=True)\n",
    "emb4 = model_polbert.encode(s4, convert_to_tensor=True)\n",
    "sim = pytorch_cos_sim(emb1, emb2)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")\n",
    "sim = pytorch_cos_sim(emb3, emb4)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")\n",
    "\n",
    "print(\"Sejmowy PolBERT\")\n",
    "emb1 = model_polsejm.encode(s1, convert_to_tensor=True)\n",
    "emb2 = model_polsejm.encode(s2, convert_to_tensor=True)\n",
    "emb3 = model_polsejm.encode(s3, convert_to_tensor=True)\n",
    "emb4 = model_polsejm.encode(s4, convert_to_tensor=True)\n",
    "sim = pytorch_cos_sim(emb1, emb2)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")\n",
    "sim = pytorch_cos_sim(emb3, emb4)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518d6ca2-bf64-4ec2-8895-cc3ab0acd9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oryginalny PolBERT\n",
      "Podobieństwo: 0.7611\n",
      "Podobieństwo: 0.7075\n",
      "Sejmowy PolBERT\n",
      "Podobieństwo: 0.7152\n",
      "Podobieństwo: 0.6758\n"
     ]
    }
   ],
   "source": [
    "s1 = \"Unia miała być wspólnotą współpracujących państw, a nie superpaństwem. Polska nie może zgodzić się na utratę prawa do samostanowienia. Bycie w UE to jedno, ale zgoda na federalizację to całkowicie inna kwestia.\"\n",
    "s2 = \"Europejska Wspólnota Węgla i Stali miała ograniczony zakres i działała w interesie gospodarki. Dzisiejsza UE poszła za daleko – regulacje, naciski polityczne, biurokracja. Nie tak miało to wyglądać.\"\n",
    "s3 = \"Unia Europejska mówi nam, co mamy robić i ogranicza naszą niezależność. To tak, jakby ktoś inny rządził naszym krajem.\"\n",
    "s4 = \"Unia Europejska to gwarancja pokoju i stabilności. Bez niej Polska byłaby samotną wyspą wśród potężnych sąsiadów.\"\n",
    "\n",
    "print(\"Oryginalny PolBERT\")\n",
    "emb1 = model_polbert.encode(s1, convert_to_tensor=True)\n",
    "emb2 = model_polbert.encode(s2, convert_to_tensor=True)\n",
    "emb3 = model_polbert.encode(s3, convert_to_tensor=True)\n",
    "emb4 = model_polbert.encode(s4, convert_to_tensor=True)\n",
    "sim = pytorch_cos_sim(emb1, emb2)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")\n",
    "sim = pytorch_cos_sim(emb3, emb4)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")\n",
    "\n",
    "print(\"Sejmowy PolBERT\")\n",
    "emb1 = model_polsejm.encode(s1, convert_to_tensor=True)\n",
    "emb2 = model_polsejm.encode(s2, convert_to_tensor=True)\n",
    "emb3 = model_polsejm.encode(s3, convert_to_tensor=True)\n",
    "emb4 = model_polsejm.encode(s4, convert_to_tensor=True)\n",
    "sim = pytorch_cos_sim(emb1, emb2)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")\n",
    "sim = pytorch_cos_sim(emb3, emb4)\n",
    "print(f\"Podobieństwo: {sim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a5d6c-2b70-4365-922f-232e8c4bb84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

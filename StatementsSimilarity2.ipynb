{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102b5be7-21d7-44f6-9c58-613984460e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in i:\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in i:\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in i:\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in i:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: sentence-transformers in i:\\anaconda\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: pocketsphinx in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (5.0.3)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pocketsphinx) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in i:\\anaconda\\lib\\site-packages (from sounddevice->pocketsphinx) (1.17.1)\n",
      "Requirement already satisfied: pycparser in i:\\anaconda\\lib\\site-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.21)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (3.11.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in i:\\anaconda\\lib\\site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in i:\\anaconda\\lib\\site-packages (from SpeechRecognition) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.12.14)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (0.2.14)\n",
      "Requirement already satisfied: tf-keras in i:\\anaconda\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in i:\\anaconda\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in i:\\anaconda\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in i:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in i:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in i:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in i:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in i:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: pyttsx3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (2.98)\n",
      "Requirement already satisfied: comtypes in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (1.4.8)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in i:\\anaconda\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Requirement already satisfied: sentence-transformers in i:\\anaconda\\lib\\site-packages (3.3.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.3.1\n",
      "    Uninstalling sentence-transformers-3.3.1:\n",
      "      Successfully uninstalled sentence-transformers-3.3.1\n",
      "Successfully installed sentence-transformers-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn\n",
    "!pip install sentence-transformers\n",
    "!pip install pocketsphinx\n",
    "!pip install SpeechRecognition\n",
    "!pip install pyaudio\n",
    "!pip install tf-keras\n",
    "!pip install pyttsx3\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4910d030-2992-4c12-ac41-e5e474bba127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Słucham...\n",
      "halo halo ha\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3 as tts\n",
    "\n",
    "r = sr.Recognizer()\n",
    "engine = tts.init()\n",
    "engine.setProperty('rate',125)\n",
    "\n",
    "def talking(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def getText():\n",
    "    with sr.Microphone() as source:\n",
    "        try:\n",
    "            print(\"Słucham...\")\n",
    "            audio = r.listen(source)\n",
    "            #text = r.recognize_sphinx(audio, language='pl')\n",
    "            text = r.recognize_google(audio, language='pl-PL')\n",
    "            if text != \"\":\n",
    "                return text\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "while True:\n",
    "    txt = getText()\n",
    "    if not txt == 0:\n",
    "        print(txt)\n",
    "        talking(txt)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Nic nie udało się rozpoznać...\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bff25ff-9197-429c-8ac7-6bea1cedecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proszę mówić...\n",
      "Błąd podczas korzystania z pocketsphinx: missing PocketSphinx language data directory: \"C:\\Users\\Sikma\\AppData\\Roaming\\Python\\Python312\\site-packages\\speech_recognition\\pocketsphinx-data\\pl\"\n"
     ]
    }
   ],
   "source": [
    "# Funkcja do rozpoznawania mowy z mikrofonu\n",
    "def speech_to_text_offline():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Proszę mówić...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "        \n",
    "        try:\n",
    "            # Użycie pocketsphinx do rozpoznawania mowy offline\n",
    "            text = recognizer.recognize_sphinx(audio, language='pl')\n",
    "            print(\"Rozpoznany tekst:\", text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Nie rozpoznano mowy\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Błąd podczas korzystania z pocketsphinx: {e}\")\n",
    "\n",
    "# Wywołanie funkcji\n",
    "speech_to_text_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8f10b4-d33b-4dcc-9035-b8d5478fc984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo halo ha\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3c4f05-b9bc-494b-b607-99c067b26ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sikma\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From I:\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from transformers import AutoModel, AutoTokenizer\n",
    "import json\n",
    "\n",
    "import nltk \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce0c1ca0-a507-44a5-8538-7870b6fa350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aborcja = pd.read_csv('datasets/aborcja.csv', encoding='utf-8', sep=';', usecols=[\"statement\", \"score\"])\n",
    "dataset_eutanazja = pd.read_csv('datasets/eutanazja.csv', encoding='utf-8', sep=';')\n",
    "dataset_bron = pd.read_csv('datasets/bron.csv', encoding='utf-8', sep=';')\n",
    "dataset_kara_smierci = pd.read_csv('datasets/kara_smierci.csv', encoding='utf-8', sep=';')\n",
    "dataset_invitro = pd.read_csv('datasets/invitro.csv', encoding='utf-8', sep=';')\n",
    "dataset_obronnosc = pd.read_csv('datasets/obronnosc.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "#dataset_euro = pd.read_csv('datasets/euro.csv', encoding='utf-8', sep=';')\n",
    "dataset_uprawnienia_policji = pd.read_csv('datasets/uprawnienia_policji.csv', encoding='utf-8', sep=';')\n",
    "dataset_sluzba_wojskowa = pd.read_csv('datasets/sluzba_wojskowa.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_samochody = pd.read_csv('datasets/samochody.csv', encoding='utf-8', sep=';')\n",
    "dataset_osiemset = pd.read_csv('datasets/osiemset.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "dataset_rownosc_plac = pd.read_csv('datasets/rownosc_plac.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "#dataset_futerkowe = pd.read_csv('datasets/futerkowe.csv', encoding='utf-8', sep=';')\n",
    "dataset_minimalna = pd.read_csv('datasets/minimalna.csv', encoding='utf-8', sep=';')\n",
    "dataset_dochodowy = pd.read_csv('datasets/dochodowy.csv', encoding='utf-8', sep=';')\n",
    "dataset_katastralny = pd.read_csv('datasets/katastralny.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_wdowia= pd.read_csv('datasets/wdowia.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "datasets = {\n",
    "    \"dataset_aborcja\": dataset_aborcja,\n",
    "    \"dataset_eutanazja\": dataset_eutanazja,\n",
    "    \"dataset_bron\": dataset_bron,\n",
    "    \"dataset_kara_smierci\": dataset_kara_smierci,\n",
    "    \"dataset_invitro\": dataset_invitro,\n",
    "    \"dataset_obronnosc\": dataset_obronnosc,\n",
    "    #\"dataset_euro\": dataset_euro,\n",
    "    \"dataset_uprawnienia_policji\": dataset_uprawnienia_policji,\n",
    "    \"dataset_sluzba_wojskowa\": dataset_sluzba_wojskowa,\n",
    "    \"dataset_samochody\": dataset_samochody,\n",
    "    \"dataset_osiemset\": dataset_osiemset,\n",
    "    \"dataset_rownosc_plac\": dataset_rownosc_plac,\n",
    "    #\"dataset_futerkowe\": dataset_futerkowe,\n",
    "    \"dataset_minimalna\": dataset_minimalna,\n",
    "    \"dataset_dochodowy\": dataset_dochodowy,\n",
    "    \"dataset_katastralny\": dataset_katastralny,\n",
    "    \"dataset_wdowia\": dataset_wdowia\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a8e6411-a453-4adf-b9d4-02d60649d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statement     object\n",
       "score        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_uprawnienia_policji.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33dc961b-0113-40cc-97c0-d9c253a2762e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia dla dataset_aborcja: -0.03772455089820366\n",
      "Mediana dla dataset_aborcja: -0.4 \n",
      "\n",
      "Średnia dla dataset_eutanazja: 0.034666666666666686\n",
      "Mediana dla dataset_eutanazja: 0.0 \n",
      "\n",
      "Średnia dla dataset_bron: -0.12537313432835823\n",
      "Mediana dla dataset_bron: -0.4 \n",
      "\n",
      "Średnia dla dataset_kara_smierci: 0.28076923076923077\n",
      "Mediana dla dataset_kara_smierci: 0.6 \n",
      "\n",
      "Średnia dla dataset_invitro: 0.10909090909090907\n",
      "Mediana dla dataset_invitro: 0.2 \n",
      "\n",
      "Średnia dla dataset_obronnosc: 0.1188235294117647\n",
      "Mediana dla dataset_obronnosc: 0.2 \n",
      "\n",
      "Średnia dla dataset_uprawnienia_policji: 0.07294117647058823\n",
      "Mediana dla dataset_uprawnienia_policji: 0.1 \n",
      "\n",
      "Średnia dla dataset_sluzba_wojskowa: 0.15857142857142856\n",
      "Mediana dla dataset_sluzba_wojskowa: 0.3 \n",
      "\n",
      "Średnia dla dataset_samochody: 0.15454545454545454\n",
      "Mediana dla dataset_samochody: 0.2 \n",
      "\n",
      "Średnia dla dataset_osiemset: 0.014516129032258067\n",
      "Mediana dla dataset_osiemset: 0.15000000000000002 \n",
      "\n",
      "Średnia dla dataset_rownosc_plac: 0.25673076923076926\n",
      "Mediana dla dataset_rownosc_plac: 0.5 \n",
      "\n",
      "Średnia dla dataset_minimalna: 0.17659574468085107\n",
      "Mediana dla dataset_minimalna: 0.3 \n",
      "\n",
      "Średnia dla dataset_dochodowy: -0.021311475409836078\n",
      "Mediana dla dataset_dochodowy: 0.0 \n",
      "\n",
      "Średnia dla dataset_katastralny: -0.11694915254237288\n",
      "Mediana dla dataset_katastralny: -0.1 \n",
      "\n",
      "Średnia dla dataset_wdowia: 0.030666666666666675\n",
      "Mediana dla dataset_wdowia: 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in datasets.items(): # iterowanie przez wszystkie pozycje klucz-wartość w słowniku. klucz to nazwy w cudzysłowie, a wartości to sam dataset.\n",
    "    print(f'Średnia dla {name}:', dataset['score'].mean())\n",
    "    print(f'Mediana dla {name}:', dataset['score'].median(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46373336-a0b6-4200-bfff-760ff8715048",
   "metadata": {},
   "source": [
    "## Wykonanie embeddingu wszystkich datasetów i zapis do plików .csv\n",
    "przeiterowanie po słowniku datasets, dokonanie embeddingów na każdym z nich oraz zapisanie odpowiednio oczyszczonych dataframeów do plików z rozszerzeniem .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b4649-31fa-42fa-a2fe-42b2bf39a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dla pojedynczego datasetu:\n",
    "dataset_embeddings = [\n",
    "    (get_embedding(row[\"statement\"]), row[\"score\"], row[\"statement\"]) \n",
    "    for _, row in dataset.iterrows() #embedding każdego datasetu\n",
    "]  \n",
    "df_dataset_embeddings = pd.DataFrame(dataset_embeddings, columns=[\"embedding\", \"score\", \"statement\"]) #konwersja zestawu danych do dataframe\n",
    "df_dataset_embeddings[\"embedding\"] = df_dataset_embeddings[\"embedding\"].apply(lambda x: \",\".join(map(str, x)))\n",
    "df_dataset_embeddings.to_csv(\"dataset_embeddings.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c9a963ea-8364-4e64-8dbb-c18daf1388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_embeddings_and_CSVsave():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset_embeddings = [\n",
    "            (get_embedding(row[\"statement\"]), row[\"score\"], row[\"statement\"]) \n",
    "            for _, row in dataset.iterrows() #embedding każdego datasetu\n",
    "        ]\n",
    "        \n",
    "        df_dataset_embeddings = pd.DataFrame(dataset_embeddings, columns=[\"embedding\", \"score\", \"statement\"]) #konwersja zestawu danych do dataframe\n",
    "\n",
    "        df_dataset_embeddings[\"embedding\"] = df_dataset_embeddings[\"embedding\"].apply(lambda x: \",\".join(map(str, x)))\n",
    "        df_dataset_embeddings.to_csv(f\"{dataset_name}_embeddings.csv\", index=False, encoding=\"utf-8\") #zapis dataframe do pliku w formacie .csv\n",
    "        \n",
    "get_datasets_embeddings_and_CSVsave()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f84683-d7ec-40bb-940a-450ee27bd53c",
   "metadata": {},
   "source": [
    "## Załadowanie wszystkich zapisanych datasetów z plików .csv\n",
    "- załadowanie wszystkich zbiorów w formacie .csv z folderu \"datasets_embeddings\" i konwersja kolumny \"embedding\" na poprawną\n",
    "- wszystkie załadowane pliki są zapisane w zmiennych globalnych o nazwach: \"df_\"+nazwa datasetu np. df_dataset_aborcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b26f06-8e95-4d7a-8c0e-eec5c409591f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dla pojedynczego datasetu\n",
    "#df_loaded = pd.read_csv(\"datasets_embeddings/dataset_bron_embeddings.csv\", encoding=\"utf-8\")\n",
    "#df_loaded[\"embedding\"] = df_loaded[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "def CSVload_datasets_embedded(directory):\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        df = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "        df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "        dataset_name = f'df_{filename.replace(\"_embeddings.csv\", \"\")}'\n",
    "        \n",
    "        globals()[dataset_name] = df\n",
    "\n",
    "CSVload_datasets_embedded(\"datasets_embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04e349bc-1eff-40c6-af63-28ecaf1a93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__session__', '_i', '_ii', '_iii', '_i1', 'SentenceTransformer', 'cosine_similarity', 'np', 'pd', 'plt', 'sns', 'json', 'nltk', 'SentimentIntensityAnalyzer', 'tqdm', 'AutoTokenizer', 'AutoModel', 'torch', 'time', 'os', '_i2', 'CSVload_datasets_embedded', 'df_dataset_aborcja', 'df_dataset_bron', 'df_dataset_dochodowy', 'df_dataset_eutanazja', 'df_dataset_invitro', 'df_dataset_kara_smierci', 'df_dataset_katastralny', 'df_dataset_minimalna', 'df_dataset_obronnosc', 'df_dataset_osiemset', 'df_dataset_rownosc_plac', 'df_dataset_samochody', 'df_dataset_sluzba_wojskowa', 'df_dataset_uprawnienia_policji', 'df_dataset_wdowia', '_i3'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f84878f-3c8d-49e6-94c2-eac069a50238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\HuggingFaceCache\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"TRANSFORMERS_CACHE\"))  # Powinno zwrócić: D:\\huggingface_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d26a73-c049-45fc-a457-18075b919889",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb06ac4d-fa6b-4b37-a626-420721a6edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8f813fe-966e-44bf-87b9-78c5fe9cd4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas ładowania modelu: 5.84 sekundy\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')\n",
    "#model = LongformerModel.from_pretrained('allenai/longformer-large-4096')\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('BAAI/bge-multilingual-gemma2') ?\n",
    "#model = SentenceTransformer('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True)\n",
    "#model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Czas ładowania modelu: {end_time - start_time:.2f} sekundy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db73148-1d46-46e7-9158-9f7ba187398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do konwersji tekstu na wektor (embedding)\n",
    "def get_embedding(text):\n",
    "    return model.encode(text)\n",
    "\n",
    "def get_similarity(user_input, df_dataset):\n",
    "    # Wektor wypowiedzi użytkownika\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    \n",
    "    # Oblicz podobieństwo dla każdej wypowiedzi w DataFrame\n",
    "    df_dataset[\"similarity\"] = df_dataset[\"embedding\"].apply(lambda emb: cosine_similarity([user_embedding], [emb])[0][0])\n",
    "    # Posortuj według podobieństwa malejąco\n",
    "    df_sorted = df_dataset.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    top_n = 4  # Weź 5 najbliższych wypowiedzi\n",
    "    top_similar = df_sorted.head(top_n)\n",
    "\n",
    "    if top_similar.iloc[0][\"similarity\"] > 0.9:\n",
    "        avg_score = top_similar.iloc[0][\"score\"]\n",
    "    else:\n",
    "        avg_score = top_similar[\"score\"].mean()\n",
    "    return avg_score, top_similar[[\"similarity\", \"score\", \"statement\"]].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68fddaa9-064f-4db6-bf14-b3b6d5b667c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_scaling\u001b[39m(x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mcopysign(math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(x)), x)\n\u001b[1;32m---> 12\u001b[0m tanh_result \u001b[38;5;241m=\u001b[39m tanh_scaling(predicted_score)\n\u001b[0;32m     13\u001b[0m power_result \u001b[38;5;241m=\u001b[39m power_scaling(predicted_score)\n\u001b[0;32m     14\u001b[0m log_result \u001b[38;5;241m=\u001b[39m log_scaling(predicted_score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_score' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def tanh_scaling(x, k=1.4):\n",
    "    return math.tanh(k * x)\n",
    "\n",
    "def power_scaling(x, b=0.5):\n",
    "    return math.copysign(abs(x) ** b, x)\n",
    "\n",
    "def log_scaling(x):\n",
    "    return math.copysign(math.log(1 + abs(x)), x)\n",
    "\n",
    "tanh_result = tanh_scaling(predicted_score)\n",
    "power_result = power_scaling(predicted_score)\n",
    "log_result = log_scaling(predicted_score)\n",
    "\n",
    "print(f\"Original value: {predicted_score}\")\n",
    "print(f\"Tanh scaling: {tanh_result}\")\n",
    "print(f\"Power scaling: {power_result}\")\n",
    "print(f\"Log scaling: {log_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f14ae-5d14-4dc3-86a0-1925ec5939d9",
   "metadata": {},
   "source": [
    "# Funkcjonalność kompasu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b9be4c-b75d-409a-a0ef-6159c5d98dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacyfizm_militaryzm = ['bron', 'obronnosc', 'sluzba_wojskowa', 'uprawnienia_policji']\n",
    "nacjonalizm_kosmopolityzm = ['obronnosc', 'sluzba_wojskowa']\n",
    "ekologia_industrializm = ['samochody', 'futerkowe']\n",
    "eurofederalizm_eurosceptyzm = ['euro']\n",
    "progresywizm_tradycjonalizm = ['aborcja', 'eutanazja', 'invitro', 'kara_smierci', 'bron', 'rownosc_plac', 'futerkowe']\n",
    "socjalizm_liberalizm = ['osiemset', 'rownosc_plac', 'futerkowe', 'minimalna', 'dochodowy', 'katastralny', 'wdowia']\n",
    "regulacjonizm_leseferyzm =  ['osiemset', 'rownosc_plac', 'minimalna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8216660d-fef6-4e8a-9c8e-abe58aae6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self):\n",
    "        self.pacyfizm_militaryzm_score = 0\n",
    "        self.pacyfizm_militaryzm_answers = 0\n",
    "        \n",
    "        self.nacjonalizm_kosmopolityzm_score = 0\n",
    "        self.nacjonalizm_kosmopolityzm_answers = 0\n",
    "\n",
    "        self.ekologia_industrializm_score = 0\n",
    "        self.ekologia_industrializm_answers = 0\n",
    "\n",
    "        self.eurofederalizm_eurosceptyzm_score = 0        \n",
    "        self.eurofederalizm_eurosceptyzm_answers = 0\n",
    "\n",
    "        self.progresywizm_tradycjonalizm_score = 0        \n",
    "        self.progresywizm_tradycjonalizm_answers = 0\n",
    "\n",
    "        self.socjalizm_liberalizm_score = 0        \n",
    "        self.socjalizm_liberalizm_answers = 0\n",
    "\n",
    "        self.regulacjonizm_leseferyzm_score = 0        \n",
    "        self.regulacjonizm_leseferyzm_answers = 0\n",
    "\n",
    "        self.nolan_gospodarka_score = 0\n",
    "        self.nolan_gospodarka_answers = 0\n",
    "\n",
    "        self.nolan_obyczajowe_score = 0\n",
    "        self.nolan_obyczajowe_answers = 0\n",
    "\n",
    "    def add_score_to_compass(self, user_statement, chosen_topic, chosen_dataset):\n",
    "        user_score, similiar_results = get_similarity(user_statement, chosen_dataset)\n",
    "\n",
    "        if chosen_topic in socjalizm_liberalizm or chosen_topic in regulacjonizm_leseferyzm:\n",
    "            if chosen_topic in socjalizm_liberalizm:\n",
    "                self.socjalizm_liberalizm_score += user_score\n",
    "                self.socjalizm_liberalizm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in regulacjonizm_leseferyzm:\n",
    "                self.regulacjonizm_leseferyzm_score += user_score\n",
    "                self.regulacjonizm_leseferyzm_answers += 1   \n",
    "                \n",
    "            self.nolan_gospodarka_score += user_score\n",
    "            self.nolan_gospodarka_answers += 1\n",
    "            \n",
    "        else:  \n",
    "            if chosen_topic in pacyfizm_militaryzm:       \n",
    "                self.pacyfizm_militaryzm_score += user_score\n",
    "                self.pacyfizm_militaryzm_answers += 1   \n",
    "    \n",
    "            if chosen_topic in nacjonalizm_kosmopolityzm:        \n",
    "                self.nacjonalizm_kosmopolityzm_score += user_score\n",
    "                self.nacjonalizm_kosmopolityzm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in ekologia_industrializm:        \n",
    "                self.ekologia_industrializm_score += user_score\n",
    "                self.ekologia_industrializm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in eurofederalizm_eurosceptyzm:       \n",
    "                self.eurofederalizm_eurosceptyzm_score += user_score\n",
    "                self.eurofederalizm_eurosceptyzm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in progresywizm_tradycjonalizm:\n",
    "                self.progresywizm_tradycjonalizm_score += user_score\n",
    "                self.progresywizm_tradycjonalizm_answers += 1\n",
    "                \n",
    "            self.nolan_obyczajowe_score += user_score\n",
    "            self.nolan_obyczajowe_answers += 1\n",
    "\n",
    "\n",
    "    def display_scores(self):\n",
    "        print(f'Wartość dla pacyfizm-militaryzm wynosi {self.pacyfizm_militaryzm_score/self.pacyfizm_militaryzm_answers}')\n",
    "        print(f'Wartość dla nacjonalizm-kosmopolityzm wynosi {self.nacjonalizm_kosmopolityzm_score/self.nacjonalizm_kosmopolityzm_answers}')\n",
    "        print(f'Wartość dla ekologia-industrializm wynosi {self.ekologia_industrializm_score/self.ekologia_industrializm_answers}')\n",
    "        print(f'Wartość dla eurofederalizm-eurosceptyzm wynosi {self.eurofederalizm_eurosceptyzm_score/self.eurofederalizm_eurosceptyzm_answers}')\n",
    "        print(f'Wartość dla progresywizm-tradycjonalizm wynosi {self.progresywizm_tradycjonalizm_score/self.progresywizm_tradycjonalizm_answers}')\n",
    "        print(f'Wartość dla socjalizm-liberalizm wynosi {self.socjalizm_liberalizm_score/self.socjalizm_liberalizm_answers}')\n",
    "        print(f'Wartość dla regulacjonizm-leseferyzm wynosi {self.regulacjonizm_leseferyzm_score/self.regulacjonizm_leseferyzm_answers}')\n",
    "        \n",
    "        print(f'Wartość dla diagramu Nolana konserwatyzm-liberalizm wynosi {self.nolan_obyczajowe_score/self.nolan_obyczajowe_answers}')\n",
    "        print(f'Wartość dla diagramu Nolana socjalizm-wolny rynek wynosi {self.nolan_gospodarka_score/self.nolan_gospodarka_answers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8acec7cd-5407-4564-92fd-a3e956e7400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = User()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f0bdbe-f474-4636-a2aa-4ee6d4fd2577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aborcja\n",
      "bron\n",
      "dochodowy\n",
      "eutanazja\n",
      "invitro\n",
      "kara_smierci\n",
      "katastralny\n",
      "minimalna\n",
      "obronnosc\n",
      "osiemset\n",
      "rownosc_plac\n",
      "samochody\n",
      "sluzba_wojskowa\n",
      "uprawnienia_policji\n",
      "wdowia\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"datasets_embeddings\"):\n",
    "    filepath = os.path.join(\"datasets_embeddings\", filename)\n",
    "\n",
    "    topic = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    topic[\"embedding\"] = topic[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "    topic_name = f'{filename.replace(\"_embeddings.csv\", \"\").replace(\"dataset_\", \"\")}'\n",
    "    print(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef23b831-9985-4181-a6e9-9427df03cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " aborcja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset o nazwie df_dataset_aborcja ZNALEZIONY.\n"
     ]
    }
   ],
   "source": [
    "chosen_topic = input()\n",
    "chosen_dataset_name = f'df_dataset_{chosen_topic}'\n",
    "if chosen_dataset_name in globals():\n",
    "    chosen_dataset = globals()[chosen_dataset_name]\n",
    "    print(f\"Dataset o nazwie {chosen_dataset_name} ZNALEZIONY.\")\n",
    "else:\n",
    "    print(f\"Dataset o nazwie {chosen_dataset_name} NIE ISTNIEJE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99a31024-a07d-4387-80d4-9a37004b57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " marzę o wyskrobaniu bachora z pizdy\n"
     ]
    }
   ],
   "source": [
    "user_statement = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbdd5dfc-2a95-4704-8bbb-cf29bfda3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user.add_score_to_compass(user_statement, chosen_topic, chosen_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c267ed1-a6a9-4afa-9085-051b6b36753d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user\u001b[38;5;241m.\u001b[39mdisplay_scores()\n",
      "Cell \u001b[1;32mIn[19], line 71\u001b[0m, in \u001b[0;36mUser.display_scores\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_scores\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWartość dla pacyfizm-militaryzm wynosi \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpacyfizm_militaryzm_score\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpacyfizm_militaryzm_answers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWartość dla nacjonalizm-kosmopolityzm wynosi \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnacjonalizm_kosmopolityzm_score\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnacjonalizm_kosmopolityzm_answers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWartość dla ekologia-industrializm wynosi \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mekologia_industrializm_score\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mekologia_industrializm_answers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "user.display_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289067f6-09af-41f1-ab1f-5871e3bcdaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be504a14-b08e-458e-af3d-ccc5e6fa1df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d45fb-82db-43aa-905b-a1ff55ec234b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa7e75-10ae-49a0-8f8a-d8881644071c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "393c6982-902e-417f-baf9-f5c21891e5cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_statement' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m avg_score, results \u001b[38;5;241m=\u001b[39m get_similarity(user_statement, chosen_dataset)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŚredni score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_score)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNajbardziej podobne wypowiedzi:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_statement' is not defined"
     ]
    }
   ],
   "source": [
    "avg_score, results = get_similarity(user_statement, chosen_dataset)\n",
    "\n",
    "print(\"Średni score:\", avg_score)\n",
    "print(\"Najbardziej podobne wypowiedzi:\")\n",
    "for sim, score, text in results:\n",
    "    print(f\"Similarity: {sim:.4f}, Score: {score}, Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "315468d9-81a6-4e62-87ef-671c2aee1e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value: -0.7\n",
      "Tanh scaling: -0.7530659048695519\n",
      "Power scaling: -0.8366600265340756\n",
      "Log scaling: -0.5306282510621704\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def tanh_scaling(x, k=1.4):\n",
    "    return math.tanh(k * x)\n",
    "\n",
    "def power_scaling(x, b=0.5):\n",
    "    return math.copysign(abs(x) ** b, x)\n",
    "\n",
    "def log_scaling(x):\n",
    "    return math.copysign(math.log(1 + abs(x)), x)\n",
    "\n",
    "tanh_result = tanh_scaling(avg_score)\n",
    "power_result = power_scaling(avg_score)\n",
    "log_result = log_scaling(avg_score)\n",
    "\n",
    "print(f\"Original value: {avg_score}\")\n",
    "print(f\"Tanh scaling: {tanh_result}\")\n",
    "print(f\"Power scaling: {power_result}\")\n",
    "print(f\"Log scaling: {log_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e1aaf-ee59-4c2f-8e09-35d04a03ab83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

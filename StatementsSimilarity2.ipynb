{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102b5be7-21d7-44f6-9c58-613984460e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in i:\\anaconda\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in i:\\anaconda\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in i:\\anaconda\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in i:\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in i:\\anaconda\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in i:\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in i:\\anaconda\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in i:\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in i:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: sentence-transformers in i:\\anaconda\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Requirement already satisfied: pocketsphinx in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (5.0.3)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pocketsphinx) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in i:\\anaconda\\lib\\site-packages (from sounddevice->pocketsphinx) (1.17.1)\n",
      "Requirement already satisfied: pycparser in i:\\anaconda\\lib\\site-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.21)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (3.11.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in i:\\anaconda\\lib\\site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in i:\\anaconda\\lib\\site-packages (from SpeechRecognition) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.12.14)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (0.2.14)\n",
      "Requirement already satisfied: tf-keras in i:\\anaconda\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in i:\\anaconda\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in i:\\anaconda\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in i:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in i:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in i:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in i:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in i:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in i:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in i:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in i:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: pyttsx3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (2.98)\n",
      "Requirement already satisfied: comtypes in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (1.4.8)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in i:\\anaconda\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Requirement already satisfied: sentence-transformers in i:\\anaconda\\lib\\site-packages (3.4.1)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.31.2)\n",
      "Requirement already satisfied: Pillow in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in i:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in i:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in i:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.3.0)\n",
      "Requirement already satisfied: colorama in i:\\anaconda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in i:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sikma\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in i:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in i:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in i:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in i:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.4.1\n",
      "    Uninstalling sentence-transformers-3.4.1:\n",
      "      Successfully uninstalled sentence-transformers-3.4.1\n",
      "Successfully installed sentence-transformers-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn\n",
    "!pip install sentence-transformers\n",
    "!pip install pocketsphinx\n",
    "!pip install SpeechRecognition\n",
    "!pip install pyaudio\n",
    "!pip install tf-keras\n",
    "!pip install pyttsx3\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4910d030-2992-4c12-ac41-e5e474bba127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Słucham...\n",
      "halo halo ha\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3 as tts\n",
    "\n",
    "r = sr.Recognizer()\n",
    "engine = tts.init()\n",
    "engine.setProperty('rate',125)\n",
    "\n",
    "def talking(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def getText():\n",
    "    with sr.Microphone() as source:\n",
    "        try:\n",
    "            print(\"Słucham...\")\n",
    "            audio = r.listen(source)\n",
    "            #text = r.recognize_sphinx(audio, language='pl')\n",
    "            text = r.recognize_google(audio, language='pl-PL')\n",
    "            if text != \"\":\n",
    "                return text\n",
    "            return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "while True:\n",
    "    txt = getText()\n",
    "    if not txt == 0:\n",
    "        print(txt)\n",
    "        talking(txt)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Nic nie udało się rozpoznać...\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bff25ff-9197-429c-8ac7-6bea1cedecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proszę mówić...\n",
      "Błąd podczas korzystania z pocketsphinx: missing PocketSphinx language data directory: \"C:\\Users\\Sikma\\AppData\\Roaming\\Python\\Python312\\site-packages\\speech_recognition\\pocketsphinx-data\\pl\"\n"
     ]
    }
   ],
   "source": [
    "# Funkcja do rozpoznawania mowy z mikrofonu\n",
    "def speech_to_text_offline():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Proszę mówić...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "        \n",
    "        try:\n",
    "            # Użycie pocketsphinx do rozpoznawania mowy offline\n",
    "            text = recognizer.recognize_sphinx(audio, language='pl')\n",
    "            print(\"Rozpoznany tekst:\", text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Nie rozpoznano mowy\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Błąd podczas korzystania z pocketsphinx: {e}\")\n",
    "\n",
    "# Wywołanie funkcji\n",
    "speech_to_text_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8f10b4-d33b-4dcc-9035-b8d5478fc984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halo halo ha\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3c4f05-b9bc-494b-b607-99c067b26ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From I:\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from transformers import AutoModel, AutoTokenizer\n",
    "import json\n",
    "\n",
    "import nltk \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0c1ca0-a507-44a5-8538-7870b6fa350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aborcja = pd.read_csv('datasets/aborcja.csv', encoding='utf-8', sep=';', usecols=[\"statement\", \"score\"])\n",
    "dataset_armia_ue = pd.read_csv('datasets/armia_ue.csv', encoding='utf-8', sep=';')\n",
    "dataset_bron = pd.read_csv('datasets/bron.csv', encoding='utf-8', sep=';')\n",
    "dataset_cpk = pd.read_csv('datasets/cpk.csv', encoding='utf-8', sep=';')\n",
    "dataset_dochodowy = pd.read_csv('datasets/dochodowy.csv', encoding='utf-8', sep=';')\n",
    "dataset_euro = pd.read_csv('datasets/euro.csv', encoding='utf-8', sep=';')\n",
    "dataset_eutanazja = pd.read_csv('datasets/eutanazja.csv', encoding='utf-8', sep=';')\n",
    "dataset_imigranci= pd.read_csv('datasets/imigranci.csv', encoding='utf-8', sep=';')\n",
    "dataset_invitro = pd.read_csv('datasets/invitro.csv', encoding='utf-8', sep=';')\n",
    "dataset_kara_smierci = pd.read_csv('datasets/kara_smierci.csv', encoding='utf-8', sep=';')\n",
    "dataset_katastralny = pd.read_csv('datasets/katastralny.csv', encoding='utf-8', sep=';')\n",
    "dataset_osiemset = pd.read_csv('datasets/osiemset.csv', encoding='utf-8', sep=';')\n",
    "dataset_samochody = pd.read_csv('datasets/samochody.csv', encoding='utf-8', sep=';')\n",
    "dataset_sluzba_wojskowa = pd.read_csv('datasets/sluzba_wojskowa.csv', encoding='utf-8', sep=';')\n",
    "dataset_ue = pd.read_csv('datasets/ue.csv', encoding='utf-8', sep=';')\n",
    "dataset_wdowia= pd.read_csv('datasets/wdowia.csv', encoding='utf-8', sep=';')\n",
    "dataset_zus = pd.read_csv('datasets/zus.csv', encoding='utf-8', sep=';')\n",
    "\n",
    "datasets = {\n",
    "    \"dataset_aborcja\": dataset_aborcja,\n",
    "    \"dataset_eutanazja\": dataset_eutanazja,\n",
    "    \"dataset_bron\": dataset_bron,\n",
    "    \"dataset_kara_smierci\": dataset_kara_smierci,\n",
    "    \"dataset_invitro\": dataset_invitro,\n",
    "    \"dataset_armia_ue\": dataset_armia_ue,\n",
    "    \"dataset_euro\": dataset_euro,\n",
    "    \"dataset_cpk\": dataset_cpk,\n",
    "    \"dataset_sluzba_wojskowa\": dataset_sluzba_wojskowa,\n",
    "    \"dataset_ue\": dataset_ue,\n",
    "    \"dataset_zus\": dataset_zus,\n",
    "    \"dataset_samochody\": dataset_samochody,\n",
    "    \"dataset_osiemset\": dataset_osiemset,\n",
    "    \"dataset_dochodowy\": dataset_dochodowy,\n",
    "    \"dataset_katastralny\": dataset_katastralny,\n",
    "    \"dataset_imigranci\": dataset_imigranci,\n",
    "    \"dataset_wdowia\": dataset_wdowia\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8e6411-a453-4adf-b9d4-02d60649d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statement     object\n",
       "score        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_aborcja.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dc961b-0113-40cc-97c0-d9c253a2762e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnia dla dataset_aborcja: -0.00466321243523319\n",
      "Mediana dla dataset_aborcja: -0.1 \n",
      "\n",
      "Średnia dla dataset_eutanazja: 0.020967741935483883\n",
      "Mediana dla dataset_eutanazja: 0.0 \n",
      "\n",
      "Średnia dla dataset_bron: 0.038095238095238085\n",
      "Mediana dla dataset_bron: 0.1 \n",
      "\n",
      "Średnia dla dataset_kara_smierci: -0.09594594594594595\n",
      "Mediana dla dataset_kara_smierci: 0.0 \n",
      "\n",
      "Średnia dla dataset_invitro: 0.0\n",
      "Mediana dla dataset_invitro: -0.1 \n",
      "\n",
      "Średnia dla dataset_armia_ue: 0.09322033898305085\n",
      "Mediana dla dataset_armia_ue: 0.2 \n",
      "\n",
      "Średnia dla dataset_euro: 0.014655172413793128\n",
      "Mediana dla dataset_euro: -0.05 \n",
      "\n",
      "Średnia dla dataset_cpk: -0.033018867924528315\n",
      "Mediana dla dataset_cpk: -0.2 \n",
      "\n",
      "Średnia dla dataset_sluzba_wojskowa: -0.01666666666666667\n",
      "Mediana dla dataset_sluzba_wojskowa: 0.0 \n",
      "\n",
      "Średnia dla dataset_ue: -0.028571428571428564\n",
      "Mediana dla dataset_ue: 0.0 \n",
      "\n",
      "Średnia dla dataset_zus: 0.018518518518518517\n",
      "Mediana dla dataset_zus: 0.05 \n",
      "\n",
      "Średnia dla dataset_samochody: 0.023684210526315797\n",
      "Mediana dla dataset_samochody: 0.1 \n",
      "\n",
      "Średnia dla dataset_osiemset: -0.01720430107526883\n",
      "Mediana dla dataset_osiemset: -0.2 \n",
      "\n",
      "Średnia dla dataset_dochodowy: 0.07758620689655174\n",
      "Mediana dla dataset_dochodowy: -0.05 \n",
      "\n",
      "Średnia dla dataset_katastralny: -0.004166666666666684\n",
      "Mediana dla dataset_katastralny: -0.1 \n",
      "\n",
      "Średnia dla dataset_imigranci: -0.04220183486238532\n",
      "Mediana dla dataset_imigranci: -0.3 \n",
      "\n",
      "Średnia dla dataset_wdowia: -0.08514851485148517\n",
      "Mediana dla dataset_wdowia: -0.4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in datasets.items(): # iterowanie przez wszystkie pozycje klucz-wartość w słowniku. klucz to nazwy w cudzysłowie, a wartości to sam dataset.\n",
    "    print(f'Średnia dla {name}:', dataset['score'].mean())\n",
    "    print(f'Mediana dla {name}:', dataset['score'].median(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46373336-a0b6-4200-bfff-760ff8715048",
   "metadata": {},
   "source": [
    "## Wykonanie embeddingu wszystkich datasetów i zapis do plików .csv\n",
    "przeiterowanie po słowniku datasets, dokonanie embeddingów na każdym z nich oraz zapisanie odpowiednio oczyszczonych dataframeów do plików z rozszerzeniem .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b4649-31fa-42fa-a2fe-42b2bf39a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dla pojedynczego datasetu:\n",
    "dataset_embeddings = [\n",
    "    (get_embedding(row[\"statement\"]), row[\"score\"], row[\"statement\"]) \n",
    "    for _, row in dataset.iterrows() #embedding każdego datasetu\n",
    "]  \n",
    "df_dataset_embeddings = pd.DataFrame(dataset_embeddings, columns=[\"embedding\", \"score\", \"statement\"]) #konwersja zestawu danych do dataframe\n",
    "df_dataset_embeddings[\"embedding\"] = df_dataset_embeddings[\"embedding\"].apply(lambda x: \",\".join(map(str, x)))\n",
    "df_dataset_embeddings.to_csv(\"dataset_embeddings.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a963ea-8364-4e64-8dbb-c18daf1388d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_embeddings_and_CSVsave():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        dataset_embeddings = [\n",
    "            (get_embedding(row[\"statement\"]), row[\"score\"], row[\"statement\"]) \n",
    "            for _, row in dataset.iterrows() #embedding każdego datasetu\n",
    "        ]\n",
    "        \n",
    "        df_dataset_embeddings = pd.DataFrame(dataset_embeddings, columns=[\"embedding\", \"score\", \"statement\"]) #konwersja zestawu danych do dataframe\n",
    "\n",
    "        df_dataset_embeddings[\"embedding\"] = df_dataset_embeddings[\"embedding\"].apply(lambda x: \",\".join(map(str, x)))\n",
    "        df_dataset_embeddings.to_csv(f\"{dataset_name}_embeddings.csv\", index=False, encoding=\"utf-8\") #zapis dataframe do pliku w formacie .csv\n",
    "        \n",
    "get_datasets_embeddings_and_CSVsave()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f84683-d7ec-40bb-940a-450ee27bd53c",
   "metadata": {},
   "source": [
    "## Załadowanie wszystkich zapisanych datasetów z plików .csv\n",
    "- załadowanie wszystkich zbiorów w formacie .csv z folderu \"datasets_embeddings\" i konwersja kolumny \"embedding\" na poprawną\n",
    "- wszystkie załadowane pliki są zapisane w zmiennych globalnych o nazwach: \"df_\"+nazwa datasetu np. df_dataset_aborcja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b26f06-8e95-4d7a-8c0e-eec5c409591f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dla pojedynczego datasetu\n",
    "#df_loaded = pd.read_csv(\"datasets_embeddings/dataset_bron_embeddings.csv\", encoding=\"utf-8\")\n",
    "#df_loaded[\"embedding\"] = df_loaded[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "def CSVload_datasets_embedded(directory):\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        df = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "        df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "        dataset_name = f'df_{filename.replace(\"_embeddings.csv\", \"\")}'\n",
    "        \n",
    "        globals()[dataset_name] = df\n",
    "\n",
    "CSVload_datasets_embedded(\"datasets_embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e349bc-1eff-40c6-af63-28ecaf1a93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__session__', 'json', 'sys', '__import__', 'NamespaceMagics', '_jupyterlab_variableinspector_nms', '_jupyterlab_variableinspector_Jupyter', '__np', '__pd', '__pyspark', '__tf', '__K', '__torch', '__ipywidgets', '__xr', '_attempt_import', '_check_imported', '_jupyterlab_variableinspector_getsizeof', '_jupyterlab_variableinspector_getshapeof', '_jupyterlab_variableinspector_getcontentof', '_jupyterlab_variableinspector_is_matrix', '_jupyterlab_variableinspector_is_widget', '_jupyterlab_variableinspector_dict_list', '_jupyterlab_variableinspector_getmatrixcontent', '_jupyterlab_variableinspector_displaywidget', '_jupyterlab_variableinspector_default', '_jupyterlab_variableinspector_deletevariable', '_1', '_i', '_ii', '_iii', '_i1', 'SentenceTransformer', 'cosine_similarity', 'np', 'pd', 'plt', 'sns', 'nltk', 'SentimentIntensityAnalyzer', 'tqdm', 'AutoTokenizer', 'AutoModel', 'torch', 'time', 'os', '_2', '_i2', 'dataset_aborcja', 'dataset_eutanazja', 'dataset_bron', 'dataset_kara_smierci', 'dataset_invitro', 'dataset_armia_ue', 'dataset_euro', 'dataset_cpk', 'dataset_sluzba_wojskowa', 'dataset_ue', 'dataset_zus', 'dataset_samochody', 'dataset_osiemset', 'dataset_dochodowy', 'dataset_katastralny', 'dataset_imigranci', 'dataset_wdowia', 'datasets', '_3', '_i3', 'CSVload_datasets_embedded', 'df_dataset_aborcja', 'df_dataset_armia_ue', 'df_dataset_bron', 'df_dataset_cpk', 'df_dataset_dochodowy', 'df_dataset_euro', 'df_dataset_eutanazja', 'df_dataset_imigranci', 'df_dataset_invitro', 'df_dataset_kara_smierci', 'df_dataset_katastralny', 'df_dataset_osiemset', 'df_dataset_samochody', 'df_dataset_sluzba_wojskowa', 'df_dataset_ue', 'df_dataset_wdowia', 'df_dataset_zus', '_4', '_i4'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f84878f-3c8d-49e6-94c2-eac069a50238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\HuggingFaceCache\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"TRANSFORMERS_CACHE\"))  # Powinno zwrócić: D:\\huggingface_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d26a73-c049-45fc-a457-18075b919889",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb06ac4d-fa6b-4b37-a626-420721a6edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f813fe-966e-44bf-87b9-78c5fe9cd4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8e7b597d9e4dee9bae91b25c5bc734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sikma\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef622f6cea544657a14908a4eff40bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/498k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0faf6e1c3af4fc9b572552e08da8ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5553084431a94fb89e5809263d89fa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in I:\\HuggingFaceCache\\models--intfloat--multilingual-e5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86da3d8b28284ba98e5b2496f9e99fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eef1e7b46441129276040b79e7a2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3d11f290854e38a2d313028b4e96a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7743548ef034bddb800777e63f02efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38f3aee041b4112ab9051d93cb59435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7f58d63b5c409d8591724003a5bb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas ładowania modelu: 124.97 sekundy\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-large-4096')\n",
    "#model = LongformerModel.from_pretrained('allenai/longformer-large-4096')\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-small')\n",
    "#model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('BAAI/bge-multilingual-gemma2') 18gb\n",
    "#model = SentenceTransformer('Alibaba-NLP/gte-multilingual-base', trust_remote_code=True)\n",
    "#model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Czas ładowania modelu: {end_time - start_time:.2f} sekundy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db73148-1d46-46e7-9158-9f7ba187398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do konwersji tekstu na wektor (embedding)\n",
    "def get_embedding(text):\n",
    "    return model.encode(text)\n",
    "\n",
    "def get_similarity(user_input, df_dataset):\n",
    "    # Wektor wypowiedzi użytkownika\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    \n",
    "    # Oblicz podobieństwo dla każdej wypowiedzi w DataFrame\n",
    "    df_dataset[\"similarity\"] = df_dataset[\"embedding\"].apply(lambda emb: cosine_similarity([user_embedding], [emb])[0][0])\n",
    "    # Posortuj według podobieństwa malejąco\n",
    "    df_sorted = df_dataset.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    top_n = 5  # Weź 5 najbliższych wypowiedzi\n",
    "    top_similar = df_sorted.head(top_n)\n",
    "\n",
    "    if top_similar.iloc[0][\"similarity\"] > 0.9:\n",
    "        avg_score = top_similar.iloc[0][\"score\"]\n",
    "    else:\n",
    "        avg_score = top_similar[\"score\"].mean()\n",
    "    return avg_score, top_similar[[\"similarity\", \"score\", \"statement\"]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f2a657-d45a-482d-8cce-85e10b945e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_upgraded(user_input, df_dataset):\n",
    "    user_embedding = get_embedding(user_input)\n",
    "    \n",
    "    df_dataset[\"similarity\"] = df_dataset[\"embedding\"].apply(lambda emb: cosine_similarity([user_embedding], [emb])[0][0])\n",
    "    df_sorted = df_dataset.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    top_n = 5\n",
    "    top_similar = df_sorted.head(top_n)\n",
    "\n",
    "    if top_similar.iloc[0][\"similarity\"] > 0.9:\n",
    "        avg_score = top_similar.iloc[0][\"score\"]\n",
    "    else:\n",
    "        avg_score = top_similar[\"score\"].mean()\n",
    "        lower_bound = max(avg_score - 0.8, -1.0)\n",
    "        upper_bound = min(avg_score + 0.8, 1.0)\n",
    "\n",
    "        df_dataset_filtered = df_dataset[(df_dataset[\"score\"] >= lower_bound) & (df_dataset[\"score\"] <= upper_bound)]\n",
    "        df_dataset_filtered = df_dataset_filtered.sort_values(by=\"similarity\", ascending=False)\n",
    "        top_similar = df_dataset_filtered.head(top_n)\n",
    "        avg_score = top_similar[\"score\"].mean()\n",
    "        \n",
    "    return avg_score, top_similar[[\"similarity\", \"score\", \"statement\"]].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68fddaa9-064f-4db6-bf14-b3b6d5b667c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_scaling\u001b[39m(x):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mcopysign(math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(x)), x)\n\u001b[1;32m---> 12\u001b[0m tanh_result \u001b[38;5;241m=\u001b[39m tanh_scaling(predicted_score)\n\u001b[0;32m     13\u001b[0m power_result \u001b[38;5;241m=\u001b[39m power_scaling(predicted_score)\n\u001b[0;32m     14\u001b[0m log_result \u001b[38;5;241m=\u001b[39m log_scaling(predicted_score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_score' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def tanh_scaling(x, k=1.4):\n",
    "    return math.tanh(k * x)\n",
    "\n",
    "def power_scaling(x, b=0.5):\n",
    "    return math.copysign(abs(x) ** b, x)\n",
    "\n",
    "def log_scaling(x):\n",
    "    return math.copysign(math.log(1 + abs(x)), x)\n",
    "\n",
    "tanh_result = tanh_scaling(predicted_score)\n",
    "power_result = power_scaling(predicted_score)\n",
    "log_result = log_scaling(predicted_score)\n",
    "\n",
    "print(f\"Original value: {predicted_score}\")\n",
    "print(f\"Tanh scaling: {tanh_result}\")\n",
    "print(f\"Power scaling: {power_result}\")\n",
    "print(f\"Log scaling: {log_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f14ae-5d14-4dc3-86a0-1925ec5939d9",
   "metadata": {},
   "source": [
    "# Funkcjonalność kompasu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b9be4c-b75d-409a-a0ef-6159c5d98dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacyfizm_militaryzm = ['bron', 'obronnosc', 'sluzba_wojskowa']\n",
    "nacjonalizm_kosmopolityzm = ['obronnosc', 'sluzba_wojskowa', 'armia_ue', 'euro', 'cpk', 'ue', 'imigranci']\n",
    "ekologia_industrializm = ['samochody', 'cpk']\n",
    "eurofederalizm_eurosceptyzm = ['euro', 'armia_ue', 'ue', 'samochody']\n",
    "progresywizm_tradycjonalizm = ['aborcja', 'eutanazja', 'invitro', 'kara_smierci', 'bron']\n",
    "socjalizm_liberalizm = ['osiemset', 'zus', 'dochodowy', 'katastralny', 'wdowia']\n",
    "regulacjonizm_leseferyzm =  ['osiemset', 'zus', 'dochodowy', 'katastralny', 'wdowia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8216660d-fef6-4e8a-9c8e-abe58aae6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self):\n",
    "        self.pacyfizm_militaryzm_score = 0\n",
    "        self.pacyfizm_militaryzm_answers = 0\n",
    "        \n",
    "        self.nacjonalizm_kosmopolityzm_score = 0\n",
    "        self.nacjonalizm_kosmopolityzm_answers = 0\n",
    "\n",
    "        self.ekologia_industrializm_score = 0\n",
    "        self.ekologia_industrializm_answers = 0\n",
    "\n",
    "        self.eurofederalizm_eurosceptyzm_score = 0        \n",
    "        self.eurofederalizm_eurosceptyzm_answers = 0\n",
    "\n",
    "        self.progresywizm_tradycjonalizm_score = 0        \n",
    "        self.progresywizm_tradycjonalizm_answers = 0\n",
    "\n",
    "        self.socjalizm_liberalizm_score = 0        \n",
    "        self.socjalizm_liberalizm_answers = 0\n",
    "\n",
    "        self.regulacjonizm_leseferyzm_score = 0        \n",
    "        self.regulacjonizm_leseferyzm_answers = 0\n",
    "\n",
    "        self.nolan_gospodarka_score = 0\n",
    "        self.nolan_gospodarka_answers = 0\n",
    "\n",
    "        self.nolan_obyczajowe_score = 0\n",
    "        self.nolan_obyczajowe_answers = 0\n",
    "\n",
    "    def add_score_to_compass(self, user_statement, chosen_topic, chosen_dataset):\n",
    "        user_score, similiar_results = get_similarity_upgraded(user_statement, chosen_dataset)\n",
    "\n",
    "        if chosen_topic in socjalizm_liberalizm or chosen_topic in regulacjonizm_leseferyzm:\n",
    "            if chosen_topic in socjalizm_liberalizm:\n",
    "                self.socjalizm_liberalizm_score += user_score\n",
    "                self.socjalizm_liberalizm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in regulacjonizm_leseferyzm:\n",
    "                self.regulacjonizm_leseferyzm_score += user_score\n",
    "                self.regulacjonizm_leseferyzm_answers += 1   \n",
    "                \n",
    "            self.nolan_gospodarka_score += user_score\n",
    "            self.nolan_gospodarka_answers += 1\n",
    "            \n",
    "        else:  \n",
    "            if chosen_topic in pacyfizm_militaryzm:       \n",
    "                self.pacyfizm_militaryzm_score += user_score\n",
    "                self.pacyfizm_militaryzm_answers += 1   \n",
    "    \n",
    "            if chosen_topic in nacjonalizm_kosmopolityzm:        \n",
    "                self.nacjonalizm_kosmopolityzm_score += user_score\n",
    "                self.nacjonalizm_kosmopolityzm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in ekologia_industrializm:        \n",
    "                self.ekologia_industrializm_score += user_score\n",
    "                self.ekologia_industrializm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in eurofederalizm_eurosceptyzm:       \n",
    "                self.eurofederalizm_eurosceptyzm_score += user_score\n",
    "                self.eurofederalizm_eurosceptyzm_answers += 1   \n",
    "                \n",
    "            if chosen_topic in progresywizm_tradycjonalizm:\n",
    "                self.progresywizm_tradycjonalizm_score += user_score\n",
    "                self.progresywizm_tradycjonalizm_answers += 1\n",
    "                \n",
    "            self.nolan_obyczajowe_score += user_score\n",
    "            self.nolan_obyczajowe_answers += 1\n",
    "\n",
    "\n",
    "    def display_scores(self):\n",
    "        if self.pacyfizm_militaryzm_answers != 0:\n",
    "            print(f'Wartość dla pacyfizm-militaryzm wynosi {self.pacyfizm_militaryzm_score/self.pacyfizm_militaryzm_answers}')\n",
    "        if self.nacjonalizm_kosmopolityzm_answers != 0:\n",
    "            print(f'Wartość dla nacjonalizm-kosmopolityzm wynosi {self.nacjonalizm_kosmopolityzm_score/self.nacjonalizm_kosmopolityzm_answers}')\n",
    "        if self.ekologia_industrializm_answers != 0:\n",
    "            print(f'Wartość dla ekologia-industrializm wynosi {self.ekologia_industrializm_score/self.ekologia_industrializm_answers}')\n",
    "        if self.eurofederalizm_eurosceptyzm_answers != 0:\n",
    "            print(f'Wartość dla eurofederalizm-eurosceptyzm wynosi {self.eurofederalizm_eurosceptyzm_score/self.eurofederalizm_eurosceptyzm_answers}')\n",
    "        if self.progresywizm_tradycjonalizm_answers != 0:\n",
    "            print(f'Wartość dla progresywizm-tradycjonalizm wynosi {self.progresywizm_tradycjonalizm_score/self.progresywizm_tradycjonalizm_answers}')\n",
    "        if self.socjalizm_liberalizm_answers != 0:\n",
    "            print(f'Wartość dla socjalizm-liberalizm wynosi {self.socjalizm_liberalizm_score/self.socjalizm_liberalizm_answers}')\n",
    "        if self.regulacjonizm_leseferyzm_answers != 0:\n",
    "            print(f'Wartość dla regulacjonizm-leseferyzm wynosi {self.regulacjonizm_leseferyzm_score/self.regulacjonizm_leseferyzm_answers}')\n",
    "        \n",
    "        if self.nolan_obyczajowe_answers != 0:\n",
    "            print(f'Wartość dla diagramu Nolana konserwatyzm-liberalizm wynosi {self.nolan_obyczajowe_score/self.nolan_obyczajowe_answers}')\n",
    "        if self.nolan_gospodarka_answers != 0:\n",
    "            print(f'Wartość dla diagramu Nolana socjalizm-wolny rynek wynosi {self.nolan_gospodarka_score/self.nolan_gospodarka_answers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8acec7cd-5407-4564-92fd-a3e956e7400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = User()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3f0bdbe-f474-4636-a2aa-4ee6d4fd2577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aborcja\n",
      "armia_ue\n",
      "bron\n",
      "cpk\n",
      "dochodowy\n",
      "euro\n",
      "eutanazja\n",
      "imigranci\n",
      "invitro\n",
      "kara_smierci\n",
      "katastralny\n",
      "osiemset\n",
      "samochody\n",
      "sluzba_wojskowa\n",
      "ue\n",
      "wdowia\n",
      "zus\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"datasets_embeddings\"):\n",
    "    filepath = os.path.join(\"datasets_embeddings\", filename)\n",
    "\n",
    "    topic = pd.read_csv(filepath, encoding=\"utf-8\")\n",
    "    topic[\"embedding\"] = topic[\"embedding\"].apply(lambda x: np.array(list(map(float, x.split(\",\")))))\n",
    "\n",
    "    topic_name = f'{filename.replace(\"_embeddings.csv\", \"\").replace(\"dataset_\", \"\")}'\n",
    "    print(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef23b831-9985-4181-a6e9-9427df03cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " bron\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset o nazwie df_dataset_bron ZNALEZIONY.\n"
     ]
    }
   ],
   "source": [
    "chosen_topic = input()\n",
    "chosen_dataset_name = f'df_dataset_{chosen_topic}'\n",
    "if chosen_dataset_name in globals():\n",
    "    chosen_dataset = globals()[chosen_dataset_name]\n",
    "    print(f\"Dataset o nazwie {chosen_dataset_name} ZNALEZIONY.\")\n",
    "else:\n",
    "    print(f\"Dataset o nazwie {chosen_dataset_name} NIE ISTNIEJE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99a31024-a07d-4387-80d4-9a37004b57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Ogólnie jestem za łatwiejszym dostępem do posiadania broni palnej, bo to oznacza większe bezpieczeństwo jednostki, ale nie chciałbym mieć u nas drugiego USA. Restrykcje i testy powinny dalej istnieć.\n"
     ]
    }
   ],
   "source": [
    "user_statement = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dbdd5dfc-2a95-4704-8bbb-cf29bfda3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user.add_score_to_compass(user_statement, chosen_topic, chosen_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c267ed1-a6a9-4afa-9085-051b6b36753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wartość dla pacyfizm-militaryzm wynosi 0.24000000000000002\n",
      "Wartość dla nacjonalizm-kosmopolityzm wynosi 0.32\n",
      "Wartość dla ekologia-industrializm wynosi 0.73\n",
      "Wartość dla eurofederalizm-eurosceptyzm wynosi 0.34\n",
      "Wartość dla progresywizm-tradycjonalizm wynosi 0.328\n",
      "Wartość dla socjalizm-liberalizm wynosi 0.26\n",
      "Wartość dla regulacjonizm-leseferyzm wynosi 0.26\n",
      "Wartość dla diagramu Nolana konserwatyzm-liberalizm wynosi 0.35999999999999993\n",
      "Wartość dla diagramu Nolana socjalizm-wolny rynek wynosi 0.26\n"
     ]
    }
   ],
   "source": [
    "user.display_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "393c6982-902e-417f-baf9-f5c21891e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni score: 0.34\n",
      "Najbardziej podobne wypowiedzi:\n",
      "Similarity: 0.7392, Score: 0.7, Text: Jestem za posiadaniem broni, ale pod warunkiem, że osoby ją kupujące przechodzą dokładne testy psychologiczne i odpowiednie szkolenia. Nie można dopuścić do sytuacji, w której ktoś niestabilny dostaje broń.\n",
      "Similarity: 0.6888, Score: 0.1, Text: Nie uważam, żeby całkowity zakaz był konieczny, ale dostęp do broni powinien być dużo trudniejszy.\n",
      "Similarity: 0.6840, Score: 0.1, Text: Obrona własna to jedno, ale broń nie powinna być zbyt łatwo dostępna.\n",
      "Similarity: 0.6821, Score: 0.3, Text: Nie jestem wielkim fanem broni, ale rozumiem ludzi, którzy chcą jej mieć dla bezpieczeństwa.\n",
      "Similarity: 0.6644, Score: 0.5, Text: Posiadanie broni powinno być przywilejem, a nie powszechnym prawem.\n"
     ]
    }
   ],
   "source": [
    "avg_score, results = get_similarity(user_statement, chosen_dataset)\n",
    "\n",
    "print(\"Średni score:\", round(avg_score, 3))\n",
    "print(\"Najbardziej podobne wypowiedzi:\")\n",
    "for sim, score, text in results:\n",
    "    print(f\"Similarity: {sim:.4f}, Score: {score}, Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ced0c79b-d152-4734-9865-c5bf6b902023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ogólnie jestem za łatwiejszym dostępem do posiadania broni palnej, bo to oznacza większe bezpieczeństwo jednostki, ale nie chciałbym mieć u nas drugiego USA. Restrykcje i testy powinny dalej istnieć.\n",
      "\n",
      "\n",
      "Średni score: 0.34\n",
      "Najbardziej podobne wypowiedzi:\n",
      "\n",
      "Similarity: 0.7392, Score: 0.7, Text: Jestem za posiadaniem broni, ale pod warunkiem, że osoby ją kupujące przechodzą dokładne testy psychologiczne i odpowiednie szkolenia. Nie można dopuścić do sytuacji, w której ktoś niestabilny dostaje broń.\n",
      "\n",
      "Similarity: 0.6888, Score: 0.1, Text: Nie uważam, żeby całkowity zakaz był konieczny, ale dostęp do broni powinien być dużo trudniejszy.\n",
      "\n",
      "Similarity: 0.6840, Score: 0.1, Text: Obrona własna to jedno, ale broń nie powinna być zbyt łatwo dostępna.\n",
      "\n",
      "Similarity: 0.6821, Score: 0.3, Text: Nie jestem wielkim fanem broni, ale rozumiem ludzi, którzy chcą jej mieć dla bezpieczeństwa.\n",
      "\n",
      "Similarity: 0.6644, Score: 0.5, Text: Posiadanie broni powinno być przywilejem, a nie powszechnym prawem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_score, results = get_similarity_upgraded(user_statement, chosen_dataset)\n",
    "\n",
    "print(user_statement)\n",
    "print('\\n')\n",
    "print(\"Średni score:\", round(avg_score, 3))\n",
    "print(\"Najbardziej podobne wypowiedzi:\\n\")\n",
    "for sim, score, text in results:\n",
    "    print(f\"Similarity: {sim:.4f}, Score: {score}, Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "315468d9-81a6-4e62-87ef-671c2aee1e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value: 0.1\n",
      "Tanh scaling: 0.139092447878458\n",
      "Power scaling: 0.31622776601683794\n",
      "Log scaling: 0.09531017980432493\n",
      "Aplify scaling: 0.1308675937775247\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "avg_score= 0.1\n",
    "\n",
    "def tanh_scaling(x, k=1.4):\n",
    "    return math.tanh(k * x)\n",
    "\n",
    "def power_scaling(x, b=0.5):\n",
    "    return math.copysign(abs(x) ** b, x)\n",
    "\n",
    "def log_scaling(x):\n",
    "    return math.copysign(math.log(1 + abs(x)), x)\n",
    "    \n",
    "def amplify(x, scale=1):\n",
    "    return math.tanh(x * scale) / math.tanh(scale)\n",
    "\n",
    "\n",
    "tanh_result = tanh_scaling(avg_score)\n",
    "power_result = power_scaling(avg_score)\n",
    "log_result = log_scaling(avg_score)\n",
    "amplify_result = amplify(avg_score)\n",
    "\n",
    "\n",
    "print(f\"Original value: {avg_score}\")\n",
    "print(f\"Tanh scaling: {tanh_result}\")\n",
    "print(f\"Power scaling: {power_result}\")\n",
    "print(f\"Log scaling: {log_result}\")\n",
    "print(f\"Aplify scaling: {amplify_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e4e1aaf-ee59-4c2f-8e09-35d04a03ab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dla 0.0 wzmocnienie wynosi 0.00, wzrost o 0.00\n",
      "Dla 0.05 wzmocnienie wynosi 0.07, wzrost o 0.02\n",
      "Dla 0.1 wzmocnienie wynosi 0.13, wzrost o 0.03\n",
      "Dla 0.15000000000000002 wzmocnienie wynosi 0.20, wzrost o 0.05\n",
      "Dla 0.2 wzmocnienie wynosi 0.26, wzrost o 0.06\n",
      "Dla 0.25 wzmocnienie wynosi 0.32, wzrost o 0.07\n",
      "Dla 0.30000000000000004 wzmocnienie wynosi 0.38, wzrost o 0.08\n",
      "Dla 0.35000000000000003 wzmocnienie wynosi 0.44, wzrost o 0.09\n",
      "Dla 0.4 wzmocnienie wynosi 0.50, wzrost o 0.10\n",
      "Dla 0.45 wzmocnienie wynosi 0.55, wzrost o 0.10\n",
      "Dla 0.5 wzmocnienie wynosi 0.61, wzrost o 0.11\n",
      "Dla 0.55 wzmocnienie wynosi 0.66, wzrost o 0.11\n",
      "Dla 0.6000000000000001 wzmocnienie wynosi 0.71, wzrost o 0.11\n",
      "Dla 0.65 wzmocnienie wynosi 0.75, wzrost o 0.10\n",
      "Dla 0.7000000000000001 wzmocnienie wynosi 0.79, wzrost o 0.09\n",
      "Dla 0.75 wzmocnienie wynosi 0.83, wzrost o 0.08\n",
      "Dla 0.8 wzmocnienie wynosi 0.87, wzrost o 0.07\n",
      "Dla 0.8500000000000001 wzmocnienie wynosi 0.91, wzrost o 0.06\n",
      "Dla 0.9 wzmocnienie wynosi 0.94, wzrost o 0.04\n",
      "Dla 0.9500000000000001 wzmocnienie wynosi 0.97, wzrost o 0.02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.arange(0, 1, 0.05):\n",
    "    print(f'Dla {i} wzmocnienie wynosi {amplify(i):.2f}, wzrost o {(amplify(i)-i):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb486a-6afd-4fb8-8cc5-03f9b6d50d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
